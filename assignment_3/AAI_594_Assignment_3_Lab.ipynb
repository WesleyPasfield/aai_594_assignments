{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAI 594 — Assignment 3\n",
    "\n",
    "## Building Agent Tools\n",
    "\n",
    "**In this lab you will:**\n",
    "- **Required (Sections 1–5):** Create **Unity Catalog function tools** — one SQL function and one Python function — and test them.\n",
    "- **Required (Section 6):** Set up **semantic search with embeddings** on the UltraFeedback dataset so an agent can find similar instructions by meaning.\n",
    "- **Required (Section 7):** Configure an **external MCP server** (You.com web search) in Databricks so your agent can access live web information.\n",
    "- **Optional, strongly encouraged (Section 8):** Create an **Agent Skill** (`SKILL.md`) that documents the tools you built.\n",
    "\n",
    "### The big picture\n",
    "\n",
    "Over Weeks 3–5 you are building an **UltraFeedback Expert** agent — an AI assistant that helps users explore and understand LLM preference data. This week you create the **tools**; next week you wire them into a working agent; in Week 5 you evaluate how well it performs.\n",
    "\n",
    "| Week | What you do | Deliverable |\n",
    "|------|------------|-------------|\n",
    "| 3 (this week) | Build tools: UC functions, semantic search, MCP | Tested tools + MCP config |\n",
    "| 4 | Wire tools into an agent; register a prompt; compare LLMs | Working agent |\n",
    "| 5 | Evaluate the agent with judges and an eval dataset | Evaluation report |\n",
    "\n",
    "**Readings this week:**\n",
    "- [Practical Guide for Agentic AI Workflows](https://arxiv.org/pdf/2512.08769)\n",
    "- [MCP Architecture](https://modelcontextprotocol.io/docs/learn/architecture)\n",
    "\n",
    "**Key docs:**\n",
    "- [Create AI agent tools with UC functions](https://docs.databricks.com/aws/en/generative-ai/agent-framework/create-custom-tool)\n",
    "- [Foundation Model APIs — Embeddings](https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html)\n",
    "- [External MCP Servers on Databricks](https://docs.databricks.com/aws/en/generative-ai/mcp/external-mcp)\n",
    "- [You.com on Databricks Marketplace](https://marketplace.databricks.com/details/32d5b7b6-0fab-4bba-9c57-5de23dd58996/Youcom_Youcom-MCP-The-1-AI-Web-Search-API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Why agents need tools *(Required)*\n",
    "\n",
    "An LLM on its own can only generate text. **Tools** give agents the ability to *act* — query databases, search the web, look up facts, run computations. In this assignment you'll create three kinds of tools:\n",
    "\n",
    "| Tool type | What it does | Example |\n",
    "|-----------|-------------|--------|\n",
    "| **UC SQL function** | Deterministic lookup against structured data | \"How many rows come from `evol_instruct`?\" |\n",
    "| **UC Python function** | Custom computation or text processing | \"Analyze the complexity of this instruction\" |\n",
    "| **Semantic search** | Embedding-based similarity search over text | \"Find instructions similar to *Explain quantum tunneling*\" |\n",
    "| **External MCP** | Access external services (web search, APIs) | \"Search the web for recent LLM benchmarks\" |\n",
    "\n",
    "Each tool is registered in a place the agent can discover it — Unity Catalog for functions and semantic search, MCP for external services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Install dependencies *(Required)*\n",
    "\n",
    "We need two packages:\n",
    "- `unitycatalog-ai[databricks]` — the Unity Catalog AI client for creating and testing UC functions as agent tools.\n",
    "- `numpy` — for computing cosine similarity between embedding vectors.\n",
    "\n",
    "**Docs:** [Unity Catalog AI](https://docs.unitycatalog.io/ai/) · [Foundation Model APIs](https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the UC AI client (for creating/testing UC functions as tools)\n",
    "# and numpy (for computing cosine similarity in semantic search)\n",
    "%pip install unitycatalog-ai[databricks] numpy\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Verify your data *(Required)*\n",
    "\n",
    "Confirm the UltraFeedback table from Assignment 1 is still available. If you get an error, re-run Assignment 1 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: confirm the table exists, show schema and row count\n",
    "df = spark.table(\"main.default.assignment_file\")\n",
    "print(f\"Row count: {df.count():,}\")\n",
    "print(f\"Columns:  {df.columns}\")\n",
    "df.printSchema()\n",
    "display(df.limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Create Unity Catalog function tools *(Required)*\n",
    "\n",
    "Unity Catalog functions are UDFs registered in `catalog.schema.function_name`. When an agent needs a tool, it calls the function by name. Two patterns are common:\n",
    "\n",
    "1. **SQL functions** — best for deterministic lookups against tables (e.g., counts, filters, joins).\n",
    "2. **Python functions** — best for custom logic, text processing, or computations that don't map cleanly to SQL.\n",
    "\n",
    "You'll create two SQL functions, a Python function, then build your own.\n",
    "\n",
    "These functions exist in Unity Catalog, so you can see them in the UI after registration\n",
    "\n",
    "**Docs:** [Create AI agent tools with UC functions](https://docs.databricks.com/aws/en/generative-ai/agent-framework/create-custom-tool) · [CREATE FUNCTION syntax](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 UC SQL functions\n",
    "\n",
    "#### all_model_combinations\n",
    "\n",
    "This function shows all combinations of chosen & rejected models, sorted by the most common combinations.\n",
    "\n",
    "**Key points:**\n",
    "- The `COMMENT` on the function and its parameters helps the agent understand *when* and *how* to use the tool. Write clear, descriptive comments.\n",
    "- The function returns a `TABLE` which has all the data in the query returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION main.default.all_model_combinations()\n",
    "RETURNS TABLE\n",
    "LANGUAGE SQL\n",
    "COMMENT 'Shows all combinations of chosen and rejected models, counts and average ratings'\n",
    "RETURN (\n",
    "  SELECT \n",
    "    `chosen-model`,\n",
    "    `rejected-model`,\n",
    "    count(source) as records,\n",
    "    avg(`chosen-rating`) as avg_chosen_rating,\n",
    "    avg(`rejected-rating`) as avg_rejected_rating\n",
    "  FROM main.default.assignment_file\n",
    "  GROUP BY 1,2\n",
    "  ORDER BY 3 desc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from main.default.all_model_combinations() limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare_models\n",
    "\n",
    "The function below compares two specific models to see how frequently an individual model wins vs. loses.\n",
    "\n",
    "This also returns a table with the count of each scenario, as well as the average winning and losing rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION main.default.compare_models(\n",
    "  model_a STRING COMMENT 'First model name to compare e.g. gpt-4',\n",
    "  model_b STRING COMMENT 'Second model name to compare'\n",
    ")\n",
    "RETURNS TABLE\n",
    "LANGUAGE SQL\n",
    "COMMENT 'Compares two models by how often each was chosen vs rejected in the UltraFeedback dataset. Returns a summary string with win counts for each model.'\n",
    "RETURN (\n",
    "  SELECT \n",
    "    CASE when `chosen-model` = model_a AND `rejected-model` = model_b THEN 'A_win'\n",
    "         when `chosen-model` = model_b AND `rejected-model` = model_a THEN 'B_win'\n",
    "         else 'other' end as comparison_scenario,\n",
    "    count(*) as win_count,\n",
    "    avg(`chosen-rating`) as avg_chosen_rating,\n",
    "    avg(`rejected-rating`) as avg_rejected_rating\n",
    "  FROM main.default.assignment_file\n",
    "  WHERE `chosen-model` IN (model_a, model_b)\n",
    "     AND `rejected-model` IN (model_a, model_b)\n",
    "  GROUP BY 1\n",
    "  ORDER BY 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select * \n",
    "from main.default.compare_models('gpt-3.5-turbo', 'alpaca-7b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Python function: `analyze_instruction`\n",
    "\n",
    "This function takes an instruction text and returns complexity metrics (word count, sentence count, estimated complexity level). An agent could use this to assess how complex a prompt is before deciding how to handle it.\n",
    "\n",
    "**Key points:**\n",
    "- Python UC functions must have **type hints** on all arguments and the return value.\n",
    "- **Imports go inside the function body** — they won't be resolved otherwise.\n",
    "- Use [Google-style docstrings](https://google.github.io/styleguide/pyguide.html#383-functions-and-methods) so the agent can parse the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "\n",
    "# Initialize the Databricks Function Client\n",
    "uc_client = DatabricksFunctionClient()\n",
    "\n",
    "# Define the Python function with type hints and a clear docstring.\n",
    "# NOTE: all imports must be INSIDE the function body.\n",
    "def analyze_instruction(instruction: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the complexity and characteristics of an instruction prompt.\n",
    "\n",
    "    Returns word count, sentence count, average word length, estimated\n",
    "    complexity level (low/medium/high), and whether the text is a question.\n",
    "    Use this to assess instruction difficulty before generating a response.\n",
    "\n",
    "    Args:\n",
    "        instruction: The instruction or prompt text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        A JSON string with analysis metrics.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import re\n",
    "\n",
    "    words = instruction.split()\n",
    "    word_count = len(words)\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', instruction) if s.strip()]\n",
    "    sentence_count = len(sentences)\n",
    "    avg_word_length = round(sum(len(w) for w in words) / max(word_count, 1), 1)\n",
    "    is_question = instruction.strip().endswith('?')\n",
    "\n",
    "    if word_count > 50 or sentence_count > 3:\n",
    "        complexity = \"high\"\n",
    "    elif word_count > 20:\n",
    "        complexity = \"medium\"\n",
    "    else:\n",
    "        complexity = \"low\"\n",
    "\n",
    "    return json.dumps({\n",
    "        \"word_count\": word_count,\n",
    "        \"sentence_count\": sentence_count,\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"complexity\": complexity,\n",
    "        \"is_question\": is_question\n",
    "    })\n",
    "\n",
    "# Register the function in Unity Catalog (main.default schema)\n",
    "function_info = uc_client.create_python_function(\n",
    "    func=analyze_instruction,\n",
    "    catalog=\"main\",\n",
    "    schema=\"default\",\n",
    "    replace=True  # overwrite if it already exists\n",
    ")\n",
    "print(f\"Registered: {function_info.full_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "\n",
    "uc_client = DatabricksFunctionClient()\n",
    "\n",
    "# Test the function out that we just created\n",
    "\n",
    "result = uc_client.execute_function(\n",
    "    function_name=\"main.default.analyze_instruction\",\n",
    "    parameters={\"instruction\": \"Heisenberg explained resonance theory, which is a structure-activity relationship that describes the effect of complementary chemical groups on a drug's properties and actions (such as solubility, absorption, and therapeutic effects). These complementary groups can be used to reduce toxicity and increase the potency of drug compounds\"})\n",
    "\n",
    "print(result.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "### 4.3 Your turn: create a function *(Required)*\n",
    "\n",
    "Create **two additional UC function** (SQL or Python or both) that would be useful for the UltraFeedback Expert agent. Some  ideas:\n",
    "\n",
    "| Idea | Type | What it does |x\n",
    "|------|------|--------------|\n",
    "| `count_model_appearances` | SQL | Count how often a model appears as chosen vs. rejected |\n",
    "| `get_sample_pairs` | SQL | Return N example chosen/rejected pairs for a given source |\n",
    "| `format_comparison` | Python | Take a chosen and rejected response and format them side-by-side |\n",
    "| `extract_keywords` | Python | Pull key terms from an instruction for categorization |\n",
    "\n",
    "Make sure your function has:\n",
    "- A clear `COMMENT` (SQL) or docstring (Python) explaining what it does and when to use it\n",
    "- Type hints (Python) or typed parameters (SQL)\n",
    "- A test cell showing it works just like the examples above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE YOUR FUNCTIONS HERE\n",
    "# Use either %%sql for a SQL function or Python with uc_client.create_python_function()\n",
    "# Then add a test cell below to verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST YOUR FUNCTIONS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. List your registered tools\n",
    "\n",
    "Before moving on, verify all your UC functions are registered. The cell below lists functions in `main.default`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "USE CATALOG main;\n",
    "SHOW USER FUNCTIONS IN main.default;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Semantic Search with Embeddings *(Required)*\n",
    "\n",
    "Semantic search lets an agent find **similar text by meaning** — not just exact keyword matches. For the UltraFeedback Expert, this means the agent can find instructions similar to a user's question, even if the wording is different.\n",
    "\n",
    "**How it works:**\n",
    "1. **Embed** each instruction using a Foundation Model embedding endpoint (`databricks-gte-large-en`).\n",
    "2. **Store** the embeddings in a Delta table.\n",
    "3. At query time, embed the user's question and compute **cosine similarity** against all stored embeddings.\n",
    "\n",
    "This approach uses the Foundation Model API (available on Free Edition) instead of Vector Search endpoints (which require a quota not available on Free Edition).\n",
    "\n",
    "> **Why not Vector Search?** Databricks Vector Search requires provisioned endpoints that exceed the Free Edition quota. The manual embedding approach teaches the same concepts (embeddings, similarity search, retrieval) and works with no extra resources.\n",
    "\n",
    "**Docs:** [Foundation Model APIs — Embeddings](https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Prepare a source table\n",
    "\n",
    "We'll create a focused table with 100 unique instructions. This keeps embedding costs low while giving the agent plenty of material to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# First, check what columns are in the source table\n",
    "af = spark.table(\"main.default.assignment_file\")\n",
    "print(f\"assignment_file columns: {af.columns}\")\n",
    "\n",
    "# Detect the instruction column name (some versions use 'instruction', others use 'prompt')\n",
    "if \"instruction\" in af.columns:\n",
    "    text_col = \"instruction\"\n",
    "elif \"prompt\" in af.columns:\n",
    "    text_col = \"prompt\"\n",
    "else:\n",
    "    raise ValueError(f\"Expected 'instruction' or 'prompt' column. Found: {af.columns}\")\n",
    "\n",
    "print(f\"Using text column: '{text_col}'\")\n",
    "\n",
    "# Create a table of 100 unique instructions for semantic search\n",
    "source_df = (\n",
    "    af.select(\"source\", af[text_col].alias(\"instruction\"))\n",
    "    .dropDuplicates([\"instruction\"])\n",
    "    .limit(100)\n",
    "    .withColumn(\"id\", monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "source_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"main.default.ultrafeedback_embeddings\")\n",
    "\n",
    "print(f\"Created table with {spark.table('main.default.ultrafeedback_embeddings').count()} rows.\")\n",
    "display(spark.table(\"main.default.ultrafeedback_embeddings\").limit(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Compute embeddings\n",
    "\n",
    "We'll use the `databricks-gte-large-en` Foundation Model endpoint to compute embeddings. The endpoint accepts batches of text and returns vectors (1024 dimensions for GTE-Large).\n",
    "\n",
    "We embed in batches because the API has input size limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "import json\n",
    "import time\n",
    "\n",
    "client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "def get_embeddings_batch(texts, endpoint=\"databricks-gte-large-en\", max_retries=5):\n",
    "    \"\"\"Embed a list of texts with retry logic for rate limits.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.predict(\n",
    "                endpoint=endpoint,\n",
    "                inputs={\"input\": texts}\n",
    "            )\n",
    "            return [item[\"embedding\"] for item in response[\"data\"]]\n",
    "        except Exception as e:\n",
    "            if \"429\" in str(e) or \"RATE\" in str(e).upper():\n",
    "                wait = 2 ** attempt * 5  # 5s, 10s, 20s, 40s, 80s\n",
    "                print(f\"  Rate limited. Waiting {wait}s (attempt {attempt+1}/{max_retries})...\")\n",
    "                time.sleep(wait)\n",
    "            else:\n",
    "                raise e\n",
    "    raise Exception(\"Max retries exceeded for embedding request.\")\n",
    "\n",
    "# Load instructions from the source table\n",
    "instructions_df = spark.table(\"main.default.ultrafeedback_embeddings\").toPandas()\n",
    "instructions = instructions_df[\"instruction\"].tolist()\n",
    "ids = instructions_df[\"id\"].tolist()\n",
    "\n",
    "print(f\"Embedding {len(instructions)} instructions (with rate-limit handling)...\")\n",
    "\n",
    "# Embed in small batches with a pause between each to avoid rate limits\n",
    "BATCH_SIZE = 5\n",
    "SLEEP_BETWEEN = 2  # seconds between batches\n",
    "all_embeddings = []\n",
    "\n",
    "for i in range(0, len(instructions), BATCH_SIZE):\n",
    "    batch = instructions[i:i + BATCH_SIZE]\n",
    "    batch_embeddings = get_embeddings_batch(batch)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    print(f\"  Embedded {min(i + BATCH_SIZE, len(instructions))}/{len(instructions)}\")\n",
    "    if i + BATCH_SIZE < len(instructions):\n",
    "        time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "print(f\"Done. {len(all_embeddings)} embeddings, {len(all_embeddings[0])} dimensions each.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, ArrayType, FloatType\n",
    "\n",
    "# Build a DataFrame with id, instruction, source, and embedding\n",
    "embedding_records = []\n",
    "for idx, (row_id, instr, emb) in enumerate(zip(ids, instructions, all_embeddings)):\n",
    "    embedding_records.append({\n",
    "        \"id\": int(row_id),\n",
    "        \"instruction\": instr,\n",
    "        \"source\": instructions_df.iloc[idx][\"source\"],\n",
    "        \"embedding\": emb\n",
    "    })\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), False),\n",
    "    StructField(\"instruction\", StringType(), False),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"embedding\", ArrayType(FloatType()), False),\n",
    "])\n",
    "\n",
    "emb_spark_df = spark.createDataFrame(embedding_records, schema=schema)\n",
    "\n",
    "# Save to Delta — this is our \"vector index\"\n",
    "emb_spark_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"main.default.ultrafeedback_embeddings\")\n",
    "\n",
    "print(f\"Saved {len(embedding_records)} embeddings to main.default.ultrafeedback_embeddings.\")\n",
    "display(spark.table(\"main.default.ultrafeedback_embeddings\").select(\"id\", \"instruction\", \"source\").limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Create a similarity search function\n",
    "\n",
    "Now we create a **SQL** UC function that the agent can call. Given a query, it uses keyword matching\n",
    "(`LIKE`) against stored instructions and returns the top matches.\n",
    "\n",
    "> **Why SQL instead of Python?** UC Python functions run in an isolated sandbox with no access to\n",
    "> SparkSession or Databricks auth. SQL functions, by contrast, can natively query Delta tables and\n",
    "> are executed by the SQL engine — no sandbox restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION main.default.search_similar_instruct(\n",
    "  query STRING COMMENT 'The search query — a word, phrase, or topic to find similar instructions in the UltraFeedback dataset. Examples: python, quantum computing, write a story.',\n",
    "  top_k INT COMMENT 'Maximum number of matching instructions to return. Use 3 for a quick look or 5 for more detail.'\n",
    ")\n",
    "RETURNS STRING\n",
    "COMMENT 'Searches UltraFeedback instructions by keyword matching and returns matching instructions with their source names. Use this to explore what kinds of prompts exist in the dataset for a given topic.'\n",
    "RETURN (\n",
    "  SELECT CONCAT(\n",
    "    'Query: ', query, ' | Showing up to ', CAST(top_k AS STRING), ' matches:\\n',\n",
    "    COALESCE(\n",
    "      ARRAY_JOIN(\n",
    "        SLICE(\n",
    "          COLLECT_LIST(CONCAT('[', source, '] ', LEFT(instruction, 200))),\n",
    "          1, top_k\n",
    "        ),\n",
    "        '\\n'\n",
    "      ),\n",
    "      'No matches found.'\n",
    "    )\n",
    "  )\n",
    "  FROM main.default.ultrafeedback_embeddings\n",
    "  WHERE LOWER(instruction) LIKE CONCAT('%', LOWER(query), '%')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Test the similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Test: find instructions about Python\n",
    "SELECT main.default.search_similar_instruct('python', 3) AS result\n",
    "UNION ALL\n",
    "SELECT main.default.search_similar_instruct('quantum', 3)\n",
    "UNION ALL\n",
    "SELECT main.default.search_similar_instruct('health benefits', 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Configure an external MCP server in Databricks *(Required)*\n",
    "\n",
    "The **Model Context Protocol (MCP)** is an open standard that lets AI agents connect to external tools and data sources. Databricks supports **external MCP servers** through managed proxy endpoints — you install them from the **Databricks Marketplace** and they become available in **AI Playground** and in your agent code.\n",
    "\n",
    "You'll install the **You.com MCP server**, which provides tools for:\n",
    "- **Web search** — search the web, news, and AI-optimized results\n",
    "- **Content extraction** — extract page content from URLs in markdown format\n",
    "\n",
    "This means your agent will be able to search the web for current information about LLMs, benchmarks, and research papers — something it can't do with just the UltraFeedback dataset.\n",
    "\n",
    "**Docs:** [External MCP Servers on Databricks](https://docs.databricks.com/aws/en/generative-ai/mcp/external-mcp) · [You.com on Databricks Marketplace](https://marketplace.databricks.com/details/32d5b7b6-0fab-4bba-9c57-5de23dd58996/Youcom_Youcom-MCP-The-1-AI-Web-Search-API) · [MCP Architecture](https://modelcontextprotocol.io/docs/learn/architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Get a You.com API key\n",
    "\n",
    "1. Go to [you.com/platform](https://you.com/platform).\n",
    "2. Sign in or create an account.\n",
    "3. Generate an API key and copy it. **Keep it safe — you'll need it in the next step.**\n",
    "\n",
    "- Note you get $100 of complimentary credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Install You.com MCP server from the Databricks Marketplace\n",
    "\n",
    "Databricks Marketplace offers curated MCP servers that you can install with a few clicks. This creates a **Unity Catalog connection** that proxies requests to the external MCP server.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. In your Databricks workspace, go to **Marketplace** → **Agents** → **MCP Servers** tab.\n",
    "2. Find **You.com** (or search for it) and click **Install**.\n",
    "3. In the installation dialog:\n",
    "   - **Connection name**: Enter a name like `youcom_connection` (this becomes the identifier in Unity Catalog).\n",
    "   - **Host / Base path**: These are pre-populated for Marketplace servers.\n",
    "   - **Bearer token**: Paste your You.com API key from step 7.1.\n",
    "4. Click **Install** to create the connection.\n",
    "\n",
    "Once installed:\n",
    "- A Unity Catalog connection is created with your MCP server details.\n",
    "- Databricks provisions a managed proxy endpoint for secure, authenticated access.\n",
    "- The proxy URL will be: `https://<workspace-hostname>/api/2.0/mcp/external/youcom_connection`\n",
    "\n",
    "> **Tip:** To view your installed MCP servers, go to your workspace → **Agents** → **MCP Servers**.\n",
    "\n",
    "> **Alternative (advanced):** You can also create a custom HTTP connection manually. See [External MCP Servers docs](https://docs.databricks.com/aws/en/generative-ai/mcp/external-mcp) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Test your MCP connection in AI Playground\n",
    "\n",
    "The fastest way to verify your MCP server is working is through **AI Playground**:\n",
    "\n",
    "1. Go to **AI Playground** in your Databricks workspace.\n",
    "2. Choose a model with the **Tools enabled** label (e.g., `databricks-meta-llama-3-3-70b-instruct`).\n",
    "3. Click **Tools** → **+ Add tool** → **MCP Servers** → **External MCP servers**.\n",
    "4. Select your `youcom_connection` (or whatever you named it).\n",
    "5. Try a query that requires live web search:\n",
    "   - *\"Search the web for the latest LLM benchmarks from 2025-2026.\"*\n",
    "   - *\"What are the top open-source LLMs released in the last 6 months?\"*\n",
    "\n",
    "The model should invoke the You.com search tool and return live web results.\n",
    "\n",
    "> **Take a screenshot** of the agent using the You.com MCP tool in AI Playground. Include it in your submission as `screenshots/mcp_you_com.png`.\n",
    "\n",
    "> **Troubleshooting:**\n",
    "> - If the MCP server doesn't appear, verify your connection was created under **Catalog** → **Connections**.\n",
    "> - Check that your You.com API key is active at [you.com/platform](https://you.com/platform).\n",
    "> - Ensure you have `USE CONNECTION` privilege on the connection (workspace admins have this by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Use the MCP server programmatically *(Preview)*\n",
    "\n",
    "Once installed, you can use the MCP server in agent code via the Databricks MCP Client:\n",
    "\n",
    "```python\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks_mcp import DatabricksMCPClient\n",
    "\n",
    "ws = WorkspaceClient()\n",
    "host = ws.config.host\n",
    "\n",
    "mcp_client = DatabricksMCPClient(\n",
    "    server_url=f\"{host}/api/2.0/mcp/external/youcom_connection\",\n",
    "    workspace_client=ws\n",
    ")\n",
    "\n",
    "# List available tools\n",
    "tools = mcp_client.list_tools()\n",
    "print([tool.name for tool in tools])\n",
    "\n",
    "# Call a search tool\n",
    "response = mcp_client.call_tool(\"you-search\", {\"query\": \"latest LLM benchmarks 2026\"})\n",
    "print(response.content[0].text)\n",
    "```\n",
    "\n",
    "You can also include external MCP servers alongside Databricks managed MCP servers:\n",
    "\n",
    "```python\n",
    "MANAGED_MCP_SERVER_URLS = [\n",
    "    f\"{host}/api/2.0/mcp/functions/system/ai\",          # Default managed MCP\n",
    "    f\"{host}/api/2.0/mcp/external/youcom_connection\"     # External MCP proxy\n",
    "]\n",
    "```\n",
    "\n",
    "We'll use this pattern in later assignments to give agents both UC tools and web search capabilities.\n",
    "\n",
    "#### Other MCP servers on the Marketplace\n",
    "\n",
    "| MCP Server | What it provides | Where to find it |\n",
    "|------------|-----------------|------------------|\n",
    "| **You.com** (required) | Web search, content extraction | [Marketplace](https://marketplace.databricks.com/details/32d5b7b6-0fab-4bba-9c57-5de23dd58996/Youcom_Youcom-MCP-The-1-AI-Web-Search-API) |\n",
    "| **GitHub** | Code search, issues, PRs | Marketplace → Agents → MCP Servers |\n",
    "| **Custom HTTP** | Any MCP server with Streamable HTTP | Manual connection setup |\n",
    "\n",
    "Browse the full catalog at **Marketplace** → **Agents** → **MCP Servers**, or create custom HTTP connections for self-hosted servers. See the [External MCP docs](https://docs.databricks.com/aws/en/generative-ai/mcp/external-mcp) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Bonus: Create an Agent Skill *(Optional, strongly encouraged)*\n",
    "\n",
    "An **Agent Skill** is a markdown document (`SKILL.md`) that gives an AI assistant domain knowledge. When a skill is loaded, the assistant knows how to use specific tools, follow procedures, and avoid common mistakes — without you having to explain everything in every prompt.\n",
    "\n",
    "Think of it as a **user manual for your agent's tools**, written so another AI can follow it.\n",
    "\n",
    "### Why this matters\n",
    "\n",
    "You've just built several tools (UC functions, semantic search, MCP). But an AI assistant doesn't automatically know *when* to use each one, *how* to call them, or *what to watch out for*. A skill bridges that gap.\n",
    "\n",
    "### Create a skill\n",
    "\n",
    "Create a file called `SKILL.md` in your `assignment_3/` folder with the following structure:\n",
    "\n",
    "```markdown\n",
    "---\n",
    "name: ultrafeedback-expert\n",
    "description: >\n",
    "  Tools and knowledge for exploring the UltraFeedback LLM preference dataset.\n",
    "  Activate when: user asks about LLM preferences, model comparisons, or\n",
    "  instruction quality in the UltraFeedback dataset.\n",
    "---\n",
    "\n",
    "# UltraFeedback Expert\n",
    "\n",
    "## When to Use This Skill\n",
    "\n",
    "**Trigger patterns:**\n",
    "- \"UltraFeedback\" or \"preference data\" or \"chosen vs rejected\"\n",
    "- \"Which model is preferred\" or \"model comparison\"\n",
    "- \"Find similar instructions\" or \"semantic search\"\n",
    "\n",
    "## Available Tools\n",
    "\n",
    "| Tool | Type | What it does |\n",
    "|------|------|--------------|\n",
    "| `main.default.lookup_source_info` | UC SQL | Returns row count and sample for a source |\n",
    "| `main.default.analyze_instruction` | UC Python | Analyzes instruction complexity |\n",
    "| `main.default.compare_models` | UC SQL | Compares two models by chosen vs rejected counts |\n",
    "| `main.default.get_model_win_rate` | UC SQL | Returns a model's win rate in the dataset |\n",
    "| `main.default.classify_instruction_topic` | UC Python | Classifies instruction topic by keywords |\n",
    "| `main.default.search_similar_instructions` | UC SQL | Keyword-based search over dataset instructions |\n",
    "| You.com MCP (Databricks) | External MCP | Live web search via Databricks proxy |\n",
    "\n",
    "## Procedures\n",
    "\n",
    "### Answering \"What sources are in the dataset?\"\n",
    "1. Call `lookup_source_info` for each known source.\n",
    "2. Summarize counts and sample instructions.\n",
    "\n",
    "### Finding similar instructions\n",
    "1. Call `search_similar_instructions` with the user's text.\n",
    "2. Return the top 3-5 matches with their sources and similarity scores.\n",
    "\n",
    "## Gotchas\n",
    "- Embeddings table (`main.default.ultrafeedback_embeddings`) must exist with pre-computed vectors.\n",
    "- Column names with hyphens (e.g., `chosen-model`) need backtick escaping.\n",
    "```\n",
    "\n",
    "Fill in the details based on the actual tools you created. You can use this skill in Cursor by placing it in `~/.cursor/skills/` or referencing it in a project rule.\n",
    "\n",
    "**Docs:** [Agent Skills standard](https://github.com/xnano-ai/agentskills) · [Cursor Rules](https://cursor.com/docs/context/rules) · [Anthropic Agent Skills Guide](https://resources.anthropic.com/hubfs/The-Complete-Guide-to-Building-Skill-for-Claude.pdf?hsLang=en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab complete\n",
    "\n",
    "### Required (Sections 1–7)\n",
    "- [ ] **Section 3:** Verified the UltraFeedback table exists.\n",
    "- [ ] **Section 4:** Created and tested the SQL function (`lookup_source_info`) and Python function (`analyze_instruction`).\n",
    "- [ ] **Section 4.3:** Created and tested your own UC function.\n",
    "- [ ] **Section 5:** Listed all registered UC functions.\n",
    "- [ ] **Section 6:** Created embeddings for 100 instructions, registered the  UC function, and tested semantic search.\n",
    "- [ ] **Section 7:** Installed the You.com MCP server from Databricks Marketplace and tested it in AI Playground (screenshot taken).\n",
    "\n",
    "### Optional but strongly encouraged (Section 8)\n",
    "- [ ] **Section 8:** Created a `SKILL.md` documenting your tools.\n",
    "\n",
    "**Submit:** Your executed notebook (`.ipynb` with all outputs) and the completed `SUBMISSION_3.md`. Include screenshots in the `screenshots/` folder.\n",
    "\n",
    "*Next week you'll wire these tools into a working agent, register a prompt, and compare different LLMs.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
