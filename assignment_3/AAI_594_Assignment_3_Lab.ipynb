{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAI 594 \u2014 Assignment 3\n",
    "\n",
    "## Building Agent Tools\n",
    "\n",
    "**In this lab you will:**\n",
    "- **Required (Sections 1\u20135):** Create **Unity Catalog function tools** \u2014 one SQL function and one Python function \u2014 and test them.\n",
    "- **Required (Section 6):** Set up **semantic search with embeddings** on the UltraFeedback dataset so an agent can find similar instructions by meaning.\n",
    "- **Required (Section 7):** Configure an **external MCP server** (You.com web search) in Cursor so your agent can access live web information.\n",
    "- **Optional, strongly encouraged (Section 8):** Create an **Agent Skill** (`SKILL.md`) that documents the tools you built.\n",
    "\n",
    "### The big picture\n",
    "\n",
    "Over Weeks 3\u20135 you are building an **UltraFeedback Expert** agent \u2014 an AI assistant that helps users explore and understand LLM preference data. This week you create the **tools**; next week you wire them into a working agent; in Week 5 you evaluate how well it performs.\n",
    "\n",
    "| Week | What you do | Deliverable |\n",
    "|------|------------|-------------|\n",
    "| 3 (this week) | Build tools: UC functions, semantic search, MCP | Tested tools + MCP config |\n",
    "| 4 | Wire tools into an agent; register a prompt; compare LLMs | Working agent |\n",
    "| 5 | Evaluate the agent with judges and an eval dataset | Evaluation report |\n",
    "\n",
    "**Readings this week:**\n",
    "- [Practical Guide for Agentic AI Workflows](https://arxiv.org/pdf/2512.08769)\n",
    "- [MCP Architecture](https://modelcontextprotocol.io/docs/learn/architecture)\n",
    "\n",
    "**Key docs:**\n",
    "- [Create AI agent tools with UC functions](https://docs.databricks.com/aws/en/generative-ai/agent-framework/create-custom-tool)\n",
    "- [Foundation Model APIs \u2014 Embeddings](https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html)\n",
    "- [You.com MCP Server](https://docs.you.com/developer-resources/mcp-server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Why agents need tools *(Required)*\n",
    "\n",
    "An LLM on its own can only generate text. **Tools** give agents the ability to *act* \u2014 query databases, search the web, look up facts, run computations. In this assignment you'll create three kinds of tools:\n",
    "\n",
    "| Tool type | What it does | Example |\n",
    "|-----------|-------------|--------|\n",
    "| **UC SQL function** | Deterministic lookup against structured data | \"How many rows come from `evol_instruct`?\" |\n",
    "| **UC Python function** | Custom computation or text processing | \"Analyze the complexity of this instruction\" |\n",
    "| **Semantic search** | Embedding-based similarity search over text | \"Find instructions similar to *Explain quantum tunneling*\" |\n",
    "| **External MCP** | Access external services (web search, APIs) | \"Search the web for recent LLM benchmarks\" |\n",
    "\n",
    "Each tool is registered in a place the agent can discover it \u2014 Unity Catalog for functions and semantic search, MCP for external services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Install dependencies *(Required)*\n",
    "\n",
    "We need two packages:\n",
    "- `unitycatalog-ai[databricks]` \u2014 the Unity Catalog AI client for creating and testing UC functions as agent tools.\n",
    "- `numpy` \u2014 for computing cosine similarity between embedding vectors.\n",
    "\n",
    "**Docs:** [Unity Catalog AI](https://docs.unitycatalog.io/ai/) \u00b7 [Foundation Model APIs](https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the UC AI client (for creating/testing UC functions as tools)\n",
    "# and numpy (for computing cosine similarity in semantic search)\n",
    "%pip install unitycatalog-ai[databricks] numpy\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Verify your data *(Required)*\n",
    "\n",
    "Confirm the UltraFeedback table from Assignment 1 is still available. If you get an error, re-run Assignment 1 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: confirm the table exists, show schema and row count\n",
    "df = spark.table(\"main.default.assignment_file\")\n",
    "print(f\"Row count: {df.count():,}\")\n",
    "print(f\"Columns:  {df.columns}\")\n",
    "df.printSchema()\n",
    "display(df.limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Create Unity Catalog function tools *(Required)*\n",
    "\n",
    "Unity Catalog functions are UDFs registered in `catalog.schema.function_name`. When an agent needs a tool, it calls the function by name. Two patterns are common:\n",
    "\n",
    "1. **SQL functions** \u2014 best for deterministic lookups against tables (e.g., counts, filters, joins).\n",
    "2. **Python functions** \u2014 best for custom logic, text processing, or computations that don't map cleanly to SQL.\n",
    "\n",
    "You'll create two SQL functions, a Python function, then build your own.\n",
    "\n",
    "These functions exist in Unity Catalog, so you can see them in the UI after registration\n",
    "\n",
    "**Docs:** [Create AI agent tools with UC functions](https://docs.databricks.com/aws/en/generative-ai/agent-framework/create-custom-tool) \u00b7 [CREATE FUNCTION syntax](https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-syntax-ddl-create-sql-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 UC SQL functions\n",
    "\n",
    "#### all_model_combinations\n",
    "\n",
    "This function shows all combinations of chosen & rejected models, sorted by the most common combinations.\n",
    "\n",
    "**Key points:**\n",
    "- The `COMMENT` on the function and its parameters helps the agent understand *when* and *how* to use the tool. Write clear, descriptive comments.\n",
    "- The function returns a `TABLE` which has all the data in the query returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION main.default.all_model_combinations()\n",
    "RETURNS TABLE\n",
    "LANGUAGE SQL\n",
    "COMMENT 'Shows all combinations of chosen and rejected models, counts and average ratings'\n",
    "RETURN (\n",
    "  SELECT \n",
    "    `chosen-model`,\n",
    "    `rejected-model`,\n",
    "    count(source) as records,\n",
    "    avg(`chosen-rating`) as avg_chosen_rating,\n",
    "    avg(`rejected-rating`) as avg_rejected_rating\n",
    "  FROM main.default.assignment_file\n",
    "  GROUP BY 1,2\n",
    "  ORDER BY 3 desc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from main.default.all_model_combinations() limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compare_models\n",
    "\n",
    "The function below compares two specific models to see how frequently an individual model wins vs. loses.\n",
    "\n",
    "This also returns a table with the count of each scenario, as well as the average winning and losing rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION main.default.compare_models(\n",
    "  model_a STRING COMMENT 'First model name to compare e.g. gpt-4',\n",
    "  model_b STRING COMMENT 'Second model name to compare'\n",
    ")\n",
    "RETURNS TABLE\n",
    "LANGUAGE SQL\n",
    "COMMENT 'Compares two models by how often each was chosen vs rejected in the UltraFeedback dataset. Returns a summary string with win counts for each model.'\n",
    "RETURN (\n",
    "  SELECT \n",
    "    CASE when `chosen-model` = model_a AND `rejected-model` = model_b THEN 'A_win'\n",
    "         when `chosen-model` = model_b AND `rejected-model` = model_a THEN 'B_win'\n",
    "         else 'other' end as comparison_scenario,\n",
    "    count(*) as win_count,\n",
    "    avg(`chosen-rating`) as avg_chosen_rating,\n",
    "    avg(`rejected-rating`) as avg_rejected_rating\n",
    "  FROM main.default.assignment_file\n",
    "  WHERE `chosen-model` IN (model_a, model_b)\n",
    "     AND `rejected-model` IN (model_a, model_b)\n",
    "  GROUP BY 1\n",
    "  ORDER BY 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select * \n",
    "from main.default.compare_models('gpt-3.5-turbo', 'alpaca-7b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Python function: `analyze_instruction`\n",
    "\n",
    "This function takes an instruction text and returns complexity metrics (word count, sentence count, estimated complexity level). An agent could use this to assess how complex a prompt is before deciding how to handle it.\n",
    "\n",
    "**Key points:**\n",
    "- Python UC functions must have **type hints** on all arguments and the return value.\n",
    "- **Imports go inside the function body** \u2014 they won't be resolved otherwise.\n",
    "- Use [Google-style docstrings](https://google.github.io/styleguide/pyguide.html#383-functions-and-methods) so the agent can parse the description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "\n",
    "# Initialize the Databricks Function Client\n",
    "uc_client = DatabricksFunctionClient()\n",
    "\n",
    "# Define the Python function with type hints and a clear docstring.\n",
    "# NOTE: all imports must be INSIDE the function body.\n",
    "def analyze_instruction(instruction: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyzes the complexity and characteristics of an instruction prompt.\n",
    "\n",
    "    Returns word count, sentence count, average word length, estimated\n",
    "    complexity level (low/medium/high), and whether the text is a question.\n",
    "    Use this to assess instruction difficulty before generating a response.\n",
    "\n",
    "    Args:\n",
    "        instruction: The instruction or prompt text to analyze.\n",
    "\n",
    "    Returns:\n",
    "        A JSON string with analysis metrics.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import re\n",
    "\n",
    "    words = instruction.split()\n",
    "    word_count = len(words)\n",
    "    sentences = [s.strip() for s in re.split(r'[.!?]+', instruction) if s.strip()]\n",
    "    sentence_count = len(sentences)\n",
    "    avg_word_length = round(sum(len(w) for w in words) / max(word_count, 1), 1)\n",
    "    is_question = instruction.strip().endswith('?')\n",
    "\n",
    "    if word_count > 50 or sentence_count > 3:\n",
    "        complexity = \"high\"\n",
    "    elif word_count > 20:\n",
    "        complexity = \"medium\"\n",
    "    else:\n",
    "        complexity = \"low\"\n",
    "\n",
    "    return json.dumps({\n",
    "        \"word_count\": word_count,\n",
    "        \"sentence_count\": sentence_count,\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"complexity\": complexity,\n",
    "        \"is_question\": is_question\n",
    "    })\n",
    "\n",
    "# Register the function in Unity Catalog (main.default schema)\n",
    "function_info = uc_client.create_python_function(\n",
    "    func=analyze_instruction,\n",
    "    catalog=\"main\",\n",
    "    schema=\"default\",\n",
    "    replace=True  # overwrite if it already exists\n",
    ")\n",
    "print(f\"Registered: {function_info.full_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unitycatalog.ai.core.databricks import DatabricksFunctionClient\n",
    "\n",
    "uc_client = DatabricksFunctionClient()\n",
    "\n",
    "# Test the function out that we just created\n",
    "\n",
    "result = uc_client.execute_function(\n",
    "    function_name=\"main.default.analyze_instruction\",\n",
    "    parameters={\"instruction\": \"Heisenberg explained resonance theory, which is a structure-activity relationship that describes the effect of complementary chemical groups on a drug's properties and actions (such as solubility, absorption, and therapeutic effects). These complementary groups can be used to reduce toxicity and increase the potency of drug compounds\"})\n",
    "\n",
    "print(result.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%md\n",
    "### 4.3 Your turn: create a function *(Required)*\n",
    "\n",
    "Create **two additional UC function** (SQL or Python or both) that would be useful for the UltraFeedback Expert agent. Some  ideas:\n",
    "\n",
    "| Idea | Type | What it does |x\n",
    "|------|------|--------------|\n",
    "| `count_model_appearances` | SQL | Count how often a model appears as chosen vs. rejected |\n",
    "| `get_sample_pairs` | SQL | Return N example chosen/rejected pairs for a given source |\n",
    "| `format_comparison` | Python | Take a chosen and rejected response and format them side-by-side |\n",
    "| `extract_keywords` | Python | Pull key terms from an instruction for categorization |\n",
    "\n",
    "Make sure your function has:\n",
    "- A clear `COMMENT` (SQL) or docstring (Python) explaining what it does and when to use it\n",
    "- Type hints (Python) or typed parameters (SQL)\n",
    "- A test cell showing it works just like the examples above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE YOUR FUNCTIONS HERE\n",
    "# Use either %%sql for a SQL function or Python with uc_client.create_python_function()\n",
    "# Then add a test cell below to verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST YOUR FUNCTIONS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. List your registered tools\n",
    "\n",
    "Before moving on, verify all your UC functions are registered. The cell below lists functions in `main.default`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- List all functions you've created in main.default\n",
    "SHOW USER FUNCTIONS IN main.default;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Semantic Search with Embeddings *(Required)*\n",
    "\n",
    "Semantic search lets an agent find **similar text by meaning** \u2014 not just exact keyword matches. For the UltraFeedback Expert, this means the agent can find instructions similar to a user's question, even if the wording is different.\n",
    "\n",
    "**How it works:**\n",
    "1. **Embed** each instruction using a Foundation Model embedding endpoint (`databricks-gte-large-en`).\n",
    "2. **Store** the embeddings in a Delta table.\n",
    "3. At query time, embed the user's question and compute **cosine similarity** against all stored embeddings.\n",
    "\n",
    "This approach uses the Foundation Model API (available on Free Edition) instead of Vector Search endpoints (which require a quota not available on Free Edition).\n",
    "\n",
    "> **Why not Vector Search?** Databricks Vector Search requires provisioned endpoints that exceed the Free Edition quota. The manual embedding approach teaches the same concepts (embeddings, similarity search, retrieval) and works with no extra resources.\n",
    "\n",
    "**Docs:** [Foundation Model APIs \u2014 Embeddings](https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Prepare a source table\n",
    "\n",
    "We'll create a focused table with 500 unique instructions. This keeps embedding costs low while giving the agent plenty of material to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Create a table of 500 unique instructions for semantic search\n",
    "try:\n",
    "    spark.table(\"main.default.ultrafeedback_embeddings\").limit(1)\n",
    "    print(\"Embeddings table already exists.\")\n",
    "except:\n",
    "    print(\"Creating source table with 500 unique instructions...\")\n",
    "    source_df = (\n",
    "        spark.table(\"main.default.assignment_file\")\n",
    "        .select(\"source\", \"instruction\")\n",
    "        .dropDuplicates([\"instruction\"])\n",
    "        .limit(500)\n",
    "        .withColumn(\"id\", monotonically_increasing_id())\n",
    "    )\n",
    "    source_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"main.default.ultrafeedback_embeddings\")\n",
    "    print(f\"Created table with {source_df.count()} rows.\")\n",
    "\n",
    "display(spark.table(\"main.default.ultrafeedback_embeddings\").limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Compute embeddings\n",
    "\n",
    "We'll use the `databricks-gte-large-en` Foundation Model endpoint to compute embeddings. The endpoint accepts batches of text and returns vectors (1024 dimensions for GTE-Large).\n",
    "\n",
    "We embed in batches because the API has input size limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "import json\n",
    "\n",
    "client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "def get_embeddings_batch(texts, endpoint=\"databricks-gte-large-en\"):\n",
    "    \"\"\"Embed a list of texts using the Foundation Model API.\"\"\"\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint,\n",
    "        inputs={\"input\": texts}\n",
    "    )\n",
    "    return [item[\"embedding\"] for item in response[\"data\"]]\n",
    "\n",
    "# Load instructions from the source table\n",
    "instructions_df = spark.table(\"main.default.ultrafeedback_embeddings\").toPandas()\n",
    "instructions = instructions_df[\"instruction\"].tolist()\n",
    "ids = instructions_df[\"id\"].tolist()\n",
    "\n",
    "print(f\"Embedding {len(instructions)} instructions...\")\n",
    "\n",
    "# Embed in batches of 20\n",
    "BATCH_SIZE = 20\n",
    "all_embeddings = []\n",
    "for i in range(0, len(instructions), BATCH_SIZE):\n",
    "    batch = instructions[i:i + BATCH_SIZE]\n",
    "    batch_embeddings = get_embeddings_batch(batch)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    if (i // BATCH_SIZE) % 5 == 0:\n",
    "        print(f\"  Embedded {min(i + BATCH_SIZE, len(instructions))}/{len(instructions)}\")\n",
    "\n",
    "print(f\"Done. Each embedding has {len(all_embeddings[0])} dimensions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, LongType, StringType, ArrayType, FloatType\n",
    "\n",
    "# Build a DataFrame with id, instruction, source, and embedding\n",
    "embedding_records = []\n",
    "for idx, (row_id, instr, emb) in enumerate(zip(ids, instructions, all_embeddings)):\n",
    "    embedding_records.append({\n",
    "        \"id\": int(row_id),\n",
    "        \"instruction\": instr,\n",
    "        \"source\": instructions_df.iloc[idx][\"source\"],\n",
    "        \"embedding\": emb\n",
    "    })\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", LongType(), False),\n",
    "    StructField(\"instruction\", StringType(), False),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"embedding\", ArrayType(FloatType()), False),\n",
    "])\n",
    "\n",
    "emb_spark_df = spark.createDataFrame(embedding_records, schema=schema)\n",
    "\n",
    "# Save to Delta \u2014 this is our \"vector index\"\n",
    "emb_spark_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"main.default.ultrafeedback_embeddings\")\n",
    "\n",
    "print(f\"Saved {len(embedding_records)} embeddings to main.default.ultrafeedback_embeddings.\")\n",
    "display(spark.table(\"main.default.ultrafeedback_embeddings\").select(\"id\", \"instruction\", \"source\").limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Create a similarity search function\n",
    "\n",
    "Now we create a Python UC function that the agent can call. Given a query, it embeds the query, loads the stored embeddings, computes cosine similarity, and returns the top matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_instructions(query: str, top_k: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Finds instructions in the UltraFeedback dataset that are semantically similar to the query.\n",
    "    Uses embedding-based cosine similarity search. Returns the top_k most similar instructions\n",
    "    with their similarity scores and source names as a JSON string.\n",
    "    Use this when a user wants to find instructions similar to a given topic or phrase.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    import mlflow.deployments\n",
    "\n",
    "    deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "\n",
    "    # Embed the query\n",
    "    response = deploy_client.predict(\n",
    "        endpoint=\"databricks-gte-large-en\",\n",
    "        inputs={\"input\": [query]}\n",
    "    )\n",
    "    query_emb = response[\"data\"][0][\"embedding\"]\n",
    "\n",
    "    # Load stored embeddings from the Delta table\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark_session = SparkSession.builder.getOrCreate()\n",
    "    emb_df = spark_session.table(\"main.default.ultrafeedback_embeddings\").toPandas()\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    import numpy as np\n",
    "    query_vec = np.array(query_emb)\n",
    "    query_norm = np.linalg.norm(query_vec)\n",
    "\n",
    "    similarities = []\n",
    "    for _, row in emb_df.iterrows():\n",
    "        doc_vec = np.array(row[\"embedding\"])\n",
    "        doc_norm = np.linalg.norm(doc_vec)\n",
    "        if query_norm == 0 or doc_norm == 0:\n",
    "            sim = 0.0\n",
    "        else:\n",
    "            sim = float(np.dot(query_vec, doc_vec) / (query_norm * doc_norm))\n",
    "        similarities.append({\n",
    "            \"instruction\": row[\"instruction\"],\n",
    "            \"source\": row[\"source\"],\n",
    "            \"similarity\": round(sim, 4)\n",
    "        })\n",
    "\n",
    "    # Sort by similarity and return top_k\n",
    "    similarities.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "    top_results = similarities[:top_k]\n",
    "\n",
    "    return json.dumps({\n",
    "        \"query\": query,\n",
    "        \"num_results\": len(top_results),\n",
    "        \"results\": top_results\n",
    "    })\n",
    "\n",
    "function_info = uc_client.create_python_function(\n",
    "    func=search_similar_instructions,\n",
    "    catalog=\"main\",\n",
    "    schema=\"default\",\n",
    "    replace=True\n",
    ")\n",
    "print(f\"Registered: {function_info.full_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Test the similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test similarity search\n",
    "import json\n",
    "\n",
    "test_queries = [\n",
    "    \"Explain quantum computing\",\n",
    "    \"Write a Python function\",\n",
    "    \"What are the health benefits of exercise?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = uc_client.execute_function(\n",
    "        function_name=\"main.default.search_similar_instructions\",\n",
    "        parameters={\"query\": query, \"top_k\": 3}\n",
    "    )\n",
    "    parsed = json.loads(result.value)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    for r in parsed[\"results\"]:\n",
    "        print(f\"  [{r['similarity']:.3f}] ({r['source']}) {r['instruction'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Configure an external MCP server *(Required)*\n",
    "\n",
    "The **Model Context Protocol (MCP)** is an open standard that lets AI assistants connect to external tools and data sources. By adding an MCP server to Cursor, your agent gains access to live capabilities \u2014 in this case, **web search**.\n",
    "\n",
    "You'll configure the **You.com MCP server**, which provides:\n",
    "- `you-search` \u2014 web and news search with filtering\n",
    "- `you-contents` \u2014 extract content from URLs in markdown format\n",
    "\n",
    "This means your agent will be able to search the web for current information about LLMs, benchmarks, and research papers \u2014 something it can't do with just the UltraFeedback dataset.\n",
    "\n",
    "**Docs:** [You.com MCP Server](https://docs.you.com/developer-resources/mcp-server) \u00b7 [MCP in Cursor](https://cursor.com/docs/context/mcp) \u00b7 [MCP Architecture](https://modelcontextprotocol.io/docs/learn/architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Get a You.com API key\n",
    "\n",
    "1. Go to [you.com/platform](https://you.com/platform).\n",
    "2. Sign in or create an account.\n",
    "3. Generate an API key and copy it. **Keep it safe \u2014 you'll need it in the next step.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Add the MCP server to Cursor\n",
    "\n",
    "Cursor reads MCP server configuration from a JSON file. You can configure it at the **project level** (only this project) or **globally** (all projects).\n",
    "\n",
    "#### Option A: Project-level (recommended for this course)\n",
    "\n",
    "Create or edit `.cursor/mcp.json` in your project root:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"ydc-server\": {\n",
    "      \"url\": \"https://api.you.com/mcp\",\n",
    "      \"headers\": {\n",
    "        \"Authorization\": \"Bearer <YOUR-YOU-COM-API-KEY>\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Replace `<YOUR-YOU-COM-API-KEY>` with your actual API key.\n",
    "\n",
    "#### Option B: Global\n",
    "\n",
    "Edit `~/.cursor/mcp.json` to make this available across all your Cursor projects.\n",
    "\n",
    "#### One-click install\n",
    "\n",
    "Alternatively, you can install directly from Cursor's MCP directory: visit the [You.com MCP page](https://docs.you.com/developer-resources/mcp-server) and click the **\"Install MCP Server\"** button for Cursor.\n",
    "\n",
    "> **Tip:** After saving `mcp.json`, restart Cursor or reload the window (`Cmd+Shift+P` \u2192 \"Reload Window\"). You should see the You.com tools available in the Agent chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Test your MCP connection\n",
    "\n",
    "Open Cursor's **Agent chat** (not the regular chat) and try a query that requires live web search:\n",
    "\n",
    "- *\"Search the web for the latest LLM benchmarks from 2025-2026.\"*\n",
    "- *\"What are the top open-source LLMs released in the last 6 months?\"*\n",
    "\n",
    "The agent should use the `you-search` tool to fetch live results. You'll see a tool-call indicator in the chat.\n",
    "\n",
    "> **Take a screenshot** of the agent using the You.com MCP tool in Cursor. Include it in your submission as `screenshots/mcp_you_com.png`.\n",
    "\n",
    "> **Troubleshooting:**\n",
    "> - If the tools don't appear, check that `mcp.json` is valid JSON (no trailing commas, correct quoting).\n",
    "> - Verify your API key is active at [you.com/platform](https://you.com/platform).\n",
    "> - Try restarting Cursor after editing the config.\n",
    "> - Go to Cursor Settings \u2192 Agents tab and turn off Cursor's built-in web search to avoid conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Alternative MCP servers\n",
    "\n",
    "You.com is the recommended MCP for this assignment, but you're welcome to configure additional servers. Here are some useful options:\n",
    "\n",
    "| MCP Server | What it provides | Get started |\n",
    "|------------|-----------------|-------------|\n",
    "| **You.com** (required) | Web search, news, content extraction | [you.com/platform](https://you.com/platform) |\n",
    "| **Brave Search** | Privacy-focused web search | [brave.com/search/api](https://brave.com/search/api/) |\n",
    "| **Tavily** | AI-optimized search for agents | [tavily.com](https://tavily.com/) |\n",
    "| **GitHub** | Code search, issues, PRs | [github.com/github/github-mcp-server](https://github.com/github/github-mcp-server) |\n",
    "| **Filesystem** | Read/write local files | Built into many MCP clients |\n",
    "\n",
    "Each follows the same pattern: get an API key, add a server entry to `mcp.json`, restart Cursor. The [MCP Registry](https://registry.modelcontextprotocol.io/) has a full catalog of available servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Bonus: Create an Agent Skill *(Optional, strongly encouraged)*\n",
    "\n",
    "An **Agent Skill** is a markdown document (`SKILL.md`) that gives an AI assistant domain knowledge. When a skill is loaded, the assistant knows how to use specific tools, follow procedures, and avoid common mistakes \u2014 without you having to explain everything in every prompt.\n",
    "\n",
    "Think of it as a **user manual for your agent's tools**, written so another AI can follow it.\n",
    "\n",
    "### Why this matters\n",
    "\n",
    "You've just built several tools (UC functions, semantic search, MCP). But an AI assistant doesn't automatically know *when* to use each one, *how* to call them, or *what to watch out for*. A skill bridges that gap.\n",
    "\n",
    "### Create a skill\n",
    "\n",
    "Create a file called `SKILL.md` in your `assignment_3/` folder with the following structure:\n",
    "\n",
    "```markdown\n",
    "---\n",
    "name: ultrafeedback-expert\n",
    "description: >\n",
    "  Tools and knowledge for exploring the UltraFeedback LLM preference dataset.\n",
    "  Activate when: user asks about LLM preferences, model comparisons, or\n",
    "  instruction quality in the UltraFeedback dataset.\n",
    "---\n",
    "\n",
    "# UltraFeedback Expert\n",
    "\n",
    "## When to Use This Skill\n",
    "\n",
    "**Trigger patterns:**\n",
    "- \"UltraFeedback\" or \"preference data\" or \"chosen vs rejected\"\n",
    "- \"Which model is preferred\" or \"model comparison\"\n",
    "- \"Find similar instructions\" or \"semantic search\"\n",
    "\n",
    "## Available Tools\n",
    "\n",
    "| Tool | Type | What it does |\n",
    "|------|------|--------------|\n",
    "| `main.default.lookup_source_info` | UC SQL | Returns row count and sample for a source |\n",
    "| `main.default.analyze_instruction` | UC Python | Analyzes instruction complexity |\n",
    "| `main.default.compare_models` | UC SQL | Compares two models by chosen vs rejected counts |\n",
    "| `main.default.get_model_win_rate` | UC SQL | Returns a model's win rate in the dataset |\n",
    "| `main.default.classify_instruction_topic` | UC Python | Classifies instruction topic by keywords |\n",
    "| `main.default.search_similar_instructions` | UC Python | Embedding-based semantic search over 500 instructions |\n",
    "| You.com MCP | External MCP | Live web search for current LLM info |\n",
    "\n",
    "## Procedures\n",
    "\n",
    "### Answering \"What sources are in the dataset?\"\n",
    "1. Call `lookup_source_info` for each known source.\n",
    "2. Summarize counts and sample instructions.\n",
    "\n",
    "### Finding similar instructions\n",
    "1. Call `search_similar_instructions` with the user's text.\n",
    "2. Return the top 3-5 matches with their sources and similarity scores.\n",
    "\n",
    "## Gotchas\n",
    "- Embeddings table (`main.default.ultrafeedback_embeddings`) must exist with pre-computed vectors.\n",
    "- Column names with hyphens (e.g., `chosen-model`) need backtick escaping.\n",
    "```\n",
    "\n",
    "Fill in the details based on the actual tools you created. You can use this skill in Cursor by placing it in `~/.cursor/skills/` or referencing it in a project rule.\n",
    "\n",
    "**Docs:** [Agent Skills standard](https://github.com/xnano-ai/agentskills) \u00b7 [Cursor Rules](https://cursor.com/docs/context/rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab complete\n",
    "\n",
    "### Required (Sections 1\u20137)\n",
    "- [ ] **Section 3:** Verified the UltraFeedback table exists.\n",
    "- [ ] **Section 4:** Created and tested the SQL function (`lookup_source_info`) and Python function (`analyze_instruction`).\n",
    "- [ ] **Section 4.3:** Created and tested your own UC function.\n",
    "- [ ] **Section 5:** Listed all registered UC functions.\n",
    "- [ ] **Section 6:** Created embeddings for 500 instructions, registered the  UC function, and tested semantic search.\n",
    "- [ ] **Section 7:** Configured the You.com MCP server in Cursor and tested it (screenshot taken).\n",
    "\n",
    "### Optional but strongly encouraged (Section 8)\n",
    "- [ ] **Section 8:** Created a `SKILL.md` documenting your tools.\n",
    "\n",
    "**Submit:** Your executed notebook (`.ipynb` with all outputs) and the completed `SUBMISSION_3.md`. Include screenshots in the `screenshots/` folder.\n",
    "\n",
    "*Next week you'll wire these tools into a working agent, register a prompt, and compare different LLMs.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}