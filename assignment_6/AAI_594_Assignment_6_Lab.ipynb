{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean up\n",
    "\n",
    "No Vector Search resources to clean up \u2014 the embeddings are stored in a Delta table (`main.default.ultrafeedback_embeddings`) which persists across assignments at no additional cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. The human-in-the-loop *(Required)*\n",
    "\n",
    "Automated judges are scalable but imperfect. They can miss domain nuances, apply criteria inconsistently, or disagree with expert opinions. **Human feedback** addresses this:\n",
    "\n",
    "- A domain expert reviews agent outputs and rates them (good/bad, with rationale)\n",
    "- The `align()` function uses this feedback to optimize the judge's prompt, making it better match human standards\n",
    "- The aligned judge can then evaluate at scale, acting as a proxy for the human expert\n",
    "\n",
    "This is the **SIMBA** (Simplified Multi-Bootstrap Aggregation) approach from the MLflow docs \u2014 it uses DSPy optimization under the hood to iteratively refine judge instructions.\n",
    "\n",
    "### Requirements for `align()`\n",
    "- A judge created with `make_judge()` (template-based)\n",
    "- At least **10 traces** with human feedback (we'll generate 15)\n",
    "- The feedback assessment name must **exactly match** the judge name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Install dependencies *(Required)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align() requires MLflow 3.4.0+\n",
    "%pip install --upgrade \"mlflow[databricks]>=3.4.0\" databricks-langchain unitycatalog-ai[databricks] numpy\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Recreate agent and custom judge *(Required)*\n",
    "\n",
    "Load everything from previous assignments: Vector Search, tools, prompt, agent, and the custom judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "## 3. Verify embeddings table *(Required)*\n",
    "\n",
    "The embeddings table (`main.default.ultrafeedback_embeddings`) was created in Assignment 3. Verify it exists and has the expected data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the embeddings table from Assignment 3 exists\n",
    "emb_df = spark.table(\"main.default.ultrafeedback_embeddings\")\n",
    "print(f\"Embeddings table has {emb_df.count()} rows.\")\n",
    "print(f\"Columns: {emb_df.columns}\")\n",
    "display(emb_df.select(\"id\", \"instruction\", \"source\").limit(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from databricks_langchain import ChatDatabricks, UCFunctionToolkit\n",
    "\n",
    "# ---- Build the agent ----\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Set an experiment for this assignment's traces\n",
    "EXPERIMENT_NAME = \"/Users/\" + spark.sql(\"SELECT current_user()\").first()[0] + \"/aai594_assignment6_align\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Load prompt\n",
    "PROMPT_NAME = \"main.default.ultrafeedback_expert_prompt\"\n",
    "try:\n",
    "    loaded_prompt = mlflow.genai.load_prompt(f\"{PROMPT_NAME}@production\")\n",
    "    SYSTEM_PROMPT = loaded_prompt.template\n",
    "    print(f\"Loaded prompt from registry.\")\n",
    "except:\n",
    "    SYSTEM_PROMPT = \"\"\"You are the UltraFeedback Expert, an AI assistant that helps users\n",
    "explore and understand the UltraFeedback LLM preference dataset.\n",
    "Use your tools to answer questions accurately.\n",
    "Always cite which tool you used and explain the results.\n",
    "If you don't have a relevant tool, say so rather than guessing.\"\"\"\n",
    "    print(\"Using fallback prompt.\")\n",
    "\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT, temperature=0.1)\n",
    "\n",
    "UC_FUNCTION_NAMES = [\n",
    "    \"main.default.lookup_source_info\",\n",
    "    \"main.default.analyze_instruction\",\n",
    "]\n",
    "toolkit = UCFunctionToolkit(function_names=UC_FUNCTION_NAMES)\n",
    "tools = toolkit.tools\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
    "\n",
    "print(\"Agent ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.judges import make_judge\n",
    "\n",
    "# Recreate the custom judge from Assignment 5\n",
    "# The name must match exactly when we log feedback later\n",
    "JUDGE_NAME = \"tool_usage_quality\"\n",
    "\n",
    "tool_usage_judge = make_judge(\n",
    "    name=JUDGE_NAME,\n",
    "    judge_prompt=\"\"\"You are evaluating an AI agent's ability to use tools appropriately.\n",
    "\n",
    "The agent has access to these tools:\n",
    "- lookup_source_info: looks up row counts and samples for a data source name\n",
    "- analyze_instruction: analyzes complexity of instruction text\n",
    "- Vector Search: finds semantically similar instructions\n",
    "\n",
    "User question: {{request}}\n",
    "Agent response: {{response}}\n",
    "\n",
    "Evaluate the agent's tool usage:\n",
    "1. Did the agent call the appropriate tool(s) for this question?\n",
    "2. If the question didn't need a tool, did the agent correctly avoid using one?\n",
    "3. Did the agent use the tool results correctly in its response?\n",
    "\n",
    "Return YES if tool usage was appropriate, NO if it was not.\n",
    "Explain your reasoning.\"\"\",\n",
    "    output_type=\"boolean\"\n",
    ")\n",
    "\n",
    "print(f\"Custom judge '{JUDGE_NAME}' created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Generate 15 traces *(Required)*\n",
    "\n",
    "Run the agent on 15 diverse questions. Each invocation is automatically logged as a **trace** in MLflow (thanks to `mlflow.langchain.autolog()`). Traces capture the full execution: inputs, outputs, tool calls, and intermediate steps.\n",
    "\n",
    "The questions cover different tool types and edge cases to give you varied outputs to review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 diverse questions for trace generation\n",
    "TRACE_QUESTIONS = [\n",
    "    # Should use lookup_source_info\n",
    "    \"How many rows come from the evol_instruct source?\",\n",
    "    \"What kind of data does the sharegpt source contain?\",\n",
    "    \"Tell me about the flan_v2 source in the dataset.\",\n",
    "    \"How does the ultrachat source compare to evol_instruct in size?\",\n",
    "    # Should use analyze_instruction\n",
    "    \"Analyze the complexity of: What is the weather today?\",\n",
    "    \"Analyze this instruction: Design a comprehensive machine learning pipeline that ingests real-time streaming data, performs feature engineering, trains multiple models in parallel, and deploys the best performer to a REST API with A/B testing.\",\n",
    "    \"How complex is: List three fruits.\",\n",
    "    # Should NOT use tools\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"Explain what a large language model is.\",\n",
    "    # Multi-step or complex\n",
    "    \"Compare evol_instruct and sharegpt: which has more data and what kind of instructions does each contain?\",\n",
    "    \"Is the evol_instruct source good for training coding assistants? Look up what it contains.\",\n",
    "    # Edge cases\n",
    "    \"Look up the source called 'nonexistent_source'\",\n",
    "    \"Analyze the complexity of an empty instruction: \",\n",
    "    \"What sources exist and how many rows does each have?\",\n",
    "    \"Can you analyze this and also look up sharegpt: Analyze complexity of 'Write a poem about AI'\",\n",
    "]\n",
    "\n",
    "# Run all queries and store results\n",
    "trace_results = []\n",
    "for i, question in enumerate(TRACE_QUESTIONS):\n",
    "    print(f\"\\n--- Trace {i+1}/15: {question[:60]}... ---\")\n",
    "    try:\n",
    "        response = agent_executor.invoke({\"input\": question})\n",
    "        output = response[\"output\"]\n",
    "        print(f\"Output: {output[:150]}...\")\n",
    "        trace_results.append({\"question\": question, \"output\": output, \"error\": None})\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        trace_results.append({\"question\": question, \"output\": None, \"error\": str(e)})\n",
    "\n",
    "print(f\"\\n=== Generated {len(trace_results)} traces ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the trace IDs from MLflow\n",
    "# These are needed to attach feedback to specific traces\n",
    "traces = mlflow.search_traces(\n",
    "    experiment_names=[EXPERIMENT_NAME],\n",
    "    max_results=15,\n",
    "    order_by=[\"timestamp_ms DESC\"]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(traces)} traces in experiment.\")\n",
    "if len(traces) > 0:\n",
    "    print(f\"\\nFirst trace ID: {traces.iloc[0]['request_id']}\")\n",
    "    print(f\"Go to the MLflow Experiments UI to view traces:\")\n",
    "    print(f\"  Sidebar > Experiments > {EXPERIMENT_NAME} > Traces tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Review traces and log feedback *(Required)*\n",
    "\n",
    "Now you become the **domain expert**. For each trace:\n",
    "\n",
    "1. **Review the trace** in the MLflow UI (Experiments > your experiment > Traces tab). Look at:\n",
    "   - Did the agent call the right tool?\n",
    "   - Was the response accurate?\n",
    "   - Did the agent handle edge cases well?\n",
    "\n",
    "2. **Log feedback** using `mlflow.log_feedback()`. For each trace, provide:\n",
    "   - `value`: `True` (good) or `False` (bad)\n",
    "   - `rationale`: Why you rated it that way (1-2 sentences)\n",
    "   - `name`: Must exactly match `\"tool_usage_quality\"` (the judge name)\n",
    "\n",
    "**Important:** The assessment name must exactly match the judge name for `align()` to work.\n",
    "\n",
    "**Docs:** [Label during development](https://docs.databricks.com/aws/en/mlflow3/genai/human-feedback/dev-annotations) \u00b7 [Feedback collection](https://mlflow.org/docs/latest/genai/assessments/feedback/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review each trace and provide your expert feedback.\n",
    "# You MUST review the actual outputs and provide honest assessments.\n",
    "#\n",
    "# INSTRUCTIONS:\n",
    "# 1. Look at the trace_results list above (question + output)\n",
    "# 2. For each, decide: did the agent use tools appropriately? (True/False)\n",
    "# 3. Write a brief rationale explaining your rating\n",
    "# 4. Run this cell to log all feedback\n",
    "#\n",
    "# Replace the placeholder feedback below with YOUR actual assessments\n",
    "# after reviewing the outputs above and in the MLflow UI.\n",
    "\n",
    "# Example feedback structure \u2014 REPLACE with your actual assessments:\n",
    "human_feedback = [\n",
    "    # Trace 1: \"How many rows come from the evol_instruct source?\"\n",
    "    {\"value\": True, \"rationale\": \"Correctly called lookup_source_info and reported the row count.\"},\n",
    "    # Trace 2: \"What kind of data does the sharegpt source contain?\"\n",
    "    {\"value\": True, \"rationale\": \"Used lookup_source_info to show a sample instruction from sharegpt.\"},\n",
    "    # Trace 3: \"Tell me about the flan_v2 source in the dataset.\"\n",
    "    {\"value\": True, \"rationale\": \"Appropriately used lookup_source_info for flan_v2.\"},\n",
    "    # Trace 4: \"How does the ultrachat source compare to evol_instruct in size?\"\n",
    "    {\"value\": True, \"rationale\": \"Called lookup_source_info for both sources and compared.\"},\n",
    "    # Trace 5: \"Analyze the complexity of: What is the weather today?\"\n",
    "    {\"value\": True, \"rationale\": \"Correctly called analyze_instruction.\"},\n",
    "    # Trace 6: Long complexity analysis\n",
    "    {\"value\": True, \"rationale\": \"Used analyze_instruction for the complex prompt.\"},\n",
    "    # Trace 7: \"How complex is: List three fruits.\"\n",
    "    {\"value\": True, \"rationale\": \"Called analyze_instruction appropriately.\"},\n",
    "    # Trace 8: \"What is the capital of Japan?\"\n",
    "    {\"value\": True, \"rationale\": \"Correctly recognized no tool was needed.\"},\n",
    "    # Trace 9: \"Explain what a large language model is.\"\n",
    "    {\"value\": True, \"rationale\": \"Answered from general knowledge without calling tools.\"},\n",
    "    # Trace 10: Compare evol_instruct and sharegpt\n",
    "    {\"value\": True, \"rationale\": \"Called lookup_source_info for both and compared them.\"},\n",
    "    # Trace 11: evol_instruct for coding assistants\n",
    "    {\"value\": True, \"rationale\": \"Looked up evol_instruct data and assessed relevance.\"},\n",
    "    # Trace 12: Nonexistent source\n",
    "    {\"value\": True, \"rationale\": \"Called the tool and handled zero results gracefully.\"},\n",
    "    # Trace 13: Empty instruction\n",
    "    {\"value\": False, \"rationale\": \"Should have noted the empty input or handled it better.\"},\n",
    "    # Trace 14: \"What sources exist and how many rows does each have?\"\n",
    "    {\"value\": False, \"rationale\": \"Only looked up one source instead of listing all.\"},\n",
    "    # Trace 15: Multi-tool query\n",
    "    {\"value\": True, \"rationale\": \"Used both analyze_instruction and lookup_source_info.\"},\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(human_feedback)} feedback entries.\")\n",
    "print(\"\\nIMPORTANT: Replace the placeholder feedback above with your actual assessments!\")\n",
    "print(\"Review each trace output in the cells above and in the MLflow UI before running the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log feedback for each trace\n",
    "# Traces are ordered most recent first, so we reverse to match our question order\n",
    "trace_ids = list(traces[\"request_id\"])\n",
    "trace_ids.reverse()  # Now trace_ids[0] corresponds to question 1\n",
    "\n",
    "logged_count = 0\n",
    "for i, (trace_id, feedback) in enumerate(zip(trace_ids[:15], human_feedback[:15])):\n",
    "    try:\n",
    "        mlflow.log_feedback(\n",
    "            trace_id=trace_id,\n",
    "            name=JUDGE_NAME,  # Must match the judge name exactly\n",
    "            value=feedback[\"value\"],\n",
    "            rationale=feedback[\"rationale\"],\n",
    "            source=mlflow.entities.feedback.FeedbackSource(\n",
    "                source_type=\"HUMAN\"\n",
    "            )\n",
    "        )\n",
    "        logged_count += 1\n",
    "        status = \"GOOD\" if feedback[\"value\"] else \"BAD\"\n",
    "        print(f\"  Trace {i+1}: [{status}] {feedback['rationale'][:60]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Trace {i+1}: Error logging feedback: {e}\")\n",
    "\n",
    "print(f\"\\nLogged feedback for {logged_count}/{len(human_feedback)} traces.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Run `align()` *(Required)*\n",
    "\n",
    "The `align()` function uses the SIMBA optimizer (built on DSPy) to improve the custom judge's prompt based on your human feedback. It iteratively refines the judge so its ratings better match yours.\n",
    "\n",
    "**Requirements:**\n",
    "- At least 10 traces with feedback (we have 15)\n",
    "- Feedback assessment name matches the judge name\n",
    "- MLflow 3.4.0+\n",
    "\n",
    "**Docs:** [Align judges with humans](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/align-judges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run alignment to optimize the judge based on human feedback\n",
    "print(\"Running align() \u2014 this may take a few minutes...\")\n",
    "print(\"(SIMBA optimizer is iterating over your feedback to improve the judge prompt)\\n\")\n",
    "\n",
    "aligned_judge = mlflow.genai.align(\n",
    "    judge=tool_usage_judge,\n",
    "    experiment_name=EXPERIMENT_NAME\n",
    ")\n",
    "\n",
    "print(\"\\nAlignment complete!\")\n",
    "print(f\"\\nOriginal judge prompt (first 200 chars):\\n{tool_usage_judge.judge_prompt[:200]}...\")\n",
    "print(f\"\\nAligned judge prompt (first 200 chars):\\n{aligned_judge.judge_prompt[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare: run both judges on the same evaluation data\n",
    "comparison_data = [\n",
    "    {\"inputs\": {\"input\": q}, \"expectations\": {}}\n",
    "    for q in TRACE_QUESTIONS[:5]  # Use first 5 questions for a quick comparison\n",
    "]\n",
    "\n",
    "def predict_fn(inputs):\n",
    "    return agent_executor.invoke(inputs)[\"output\"]\n",
    "\n",
    "print(\"Running original judge...\")\n",
    "original_results = mlflow.genai.evaluate(\n",
    "    data=comparison_data,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[tool_usage_judge]\n",
    ")\n",
    "\n",
    "print(\"\\nRunning aligned judge...\")\n",
    "aligned_results = mlflow.genai.evaluate(\n",
    "    data=comparison_data,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[aligned_judge]\n",
    ")\n",
    "\n",
    "print(\"\\nComparison complete. Check the MLflow UI to compare the two evaluation runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Reflection *(Required)*\n",
    "\n",
    "### 7a. How did alignment change the judge?\n",
    "\n",
    "*Compare the original and aligned judge prompts. What changed? Did the aligned judge add new criteria, change the emphasis, or use different language?*\n",
    "\n",
    "**[Your answer here]**\n",
    "\n",
    "### 7b. Where did the original judge disagree with your feedback?\n",
    "\n",
    "*Look at the comparison results. Were there cases where the original judge rated differently from your human feedback? What did the aligned judge do in those cases?*\n",
    "\n",
    "**[Your answer here]**\n",
    "\n",
    "### 7c. What does this teach about human-in-the-loop evaluation?\n",
    "\n",
    "*Reflect on the entire arc: automated judges (Week 5) \u2192 human feedback \u2192 alignment (this week). Why isn't just one sufficient? Connect to the \"Who Validates the Validators\" reading.*\n",
    "\n",
    "**[Your answer here]**\n",
    "\n",
    "### 7d. Looking ahead: how would you use this in your final project?\n",
    "\n",
    "*If you were to apply the trace \u2192 feedback \u2192 align workflow to your final project agent, how would you design it? What would your judge evaluate? Who would be the domain expert?*\n",
    "\n",
    "**[Your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Clean up *(Required)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Bonus: Deploy to a serving endpoint *(Optional, strongly encouraged)*\n",
    "\n",
    "On a paid Databricks workspace, you would deploy the agent using `databricks.agents.deploy()`, which creates a serving endpoint and a **Review App** for collecting stakeholder feedback through a UI.\n",
    "\n",
    "On **Free Edition**, this may not work (Agent Bricks is listed as unsupported). But you can try \u2014 model serving endpoints themselves are available with limits.\n",
    "\n",
    "If deployment fails, that's OK \u2014 the notebook-based approach above covers the same concepts.\n",
    "\n",
    "### Conceptual walkthrough\n",
    "\n",
    "In a production Databricks workspace, the workflow would be:\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "from databricks import agents\n",
    "\n",
    "# 1. Log the agent as an MLflow model\n",
    "with mlflow.start_run():\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        lc_model=agent_executor,\n",
    "        artifact_path=\"agent\",\n",
    "        registered_model_name=\"main.default.ultrafeedback_expert\"\n",
    "    )\n",
    "\n",
    "# 2. Deploy to a serving endpoint (creates Review App automatically)\n",
    "deployment = agents.deploy(\n",
    "    model_name=\"main.default.ultrafeedback_expert\",\n",
    "    model_version=model_info.registered_model_version\n",
    ")\n",
    "\n",
    "# 3. Share the Review App URL with domain experts\n",
    "print(f\"Review App: {deployment.review_app_url}\")\n",
    "\n",
    "# 4. Experts interact with the agent and leave feedback\n",
    "# 5. Run align() on the production traces\n",
    "```\n",
    "\n",
    "The Review App provides a chat interface where experts can rate responses, leave comments, and provide expected outputs \u2014 all of which feed into `align()`. The notebook approach we used in Sections 4-6 covers the same programmatic workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab complete\n",
    "\n",
    "### Required (Sections 1\u20138)\n",
    "- [ ] **Section 3:** Agent and custom judge recreated. Vector Search working.\n",
    "- [ ] **Section 4:** 15 traces generated and visible in MLflow Experiments.\n",
    "- [ ] **Section 5:** Human feedback logged for all 15 traces with honest ratings and rationales.\n",
    "- [ ] **Section 6:** `align()` ran successfully. Original and aligned judge compared.\n",
    "- [ ] **Section 7:** All four reflection questions answered.\n",
    "- [ ] **Section 8:** No VS cleanup needed (embeddings persist in Delta table).\n",
    "\n",
    "### Optional but strongly encouraged (Section 9)\n",
    "- [ ] **Section 9:** Attempted deployment or understood the production workflow.\n",
    "\n",
    "**Submit:** Your executed notebook (`.ipynb` with all outputs) and the completed `SUBMISSION_6.md`.\n",
    "\n",
    "*Congratulations! You've completed the full agent lifecycle: tools \u2192 agent \u2192 evaluation \u2192 human feedback \u2192 judge alignment. Weeks 7-8 are dedicated to your final project.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}