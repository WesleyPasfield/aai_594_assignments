{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AAI 594 — Assignment 2\n",
        "\n",
        "## Exploratory Data Analysis with the Data Science Agent\n",
        "\n",
        "**In this lab you will:**\n",
        "- **Required:** Use the Databricks **Data Science Agent** (Agent Mode in the Assistant) to perform exploratory data analysis on the UltraFeedback dataset you loaded in Assignment 1.\n",
        "- **Required:** Review the agent's output, identify what the data contains, and assess how it could be integrated into an agent (e.g., as tools, reference data, or evaluation benchmarks).\n",
        "- **Required:** Provide your own written analysis of the data — the agent helps you explore, but **your interpretation is the deliverable**.\n",
        "\n",
        "*This assignment is intentionally bare-bones. The notebook sets up the data reference; you drive the exploration through the Agent and record your findings below.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. About this assignment\n",
        "\n",
        "Last week you loaded the **UltraFeedback** dataset into Unity Catalog as `main.default.assignment_file`. This week you'll explore that data in depth.\n",
        "\n",
        "Instead of writing all EDA code by hand, you'll use the **Databricks Data Science Agent** — an AI assistant built into your notebook that can plan, generate, and run code on your behalf. Your job is to **guide the agent with good prompts**, **review what it produces**, and **write your own analysis** of the results.\n",
        "\n",
        "### Why this matters\n",
        "\n",
        "In agentic AI, the human-in-the-loop is critical. An agent can automate mechanical tasks (generating plots, computing statistics), but **you** are responsible for interpreting results, catching errors, and making decisions. This assignment practices that skill: you'll delegate the exploration to an agent, then own the analysis.\n",
        "\n",
        "### How to use the Data Science Agent\n",
        "\n",
        "1. Open the **Assistant** side panel in this notebook (click the Assistant icon or press **Cmd+I** / **Ctrl+I**).\n",
        "2. In the bottom-right corner of the panel, toggle to **Agent** mode.\n",
        "3. Type a prompt. Reference the table with `@main.default.assignment_file` so the agent knows which data to use.\n",
        "4. The agent will create a plan, ask clarifying questions, and generate notebook cells. **Review each step** before clicking **Allow** or **Continue**.\n",
        "5. After the agent finishes, read the generated cells and outputs. Add your own markdown cells with your interpretation.\n",
        "\n",
        "**Docs:** [Use the Data Science Agent](https://docs.databricks.com/aws/en/notebooks/ds-agent)\n",
        "\n",
        "**Requirements:**\n",
        "- Partner-powered AI features must be enabled for your workspace.\n",
        "- Databricks Assistant Agent Mode preview must be enabled. See [Manage Databricks previews](https://docs.databricks.com/aws/en/admin/workspace-settings/manage-previews).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Verify the dataset *(Required)*\n",
        "\n",
        "Run the cell below to confirm the table from Assignment 1 is still available and to see a quick preview. If you get an error, re-run Assignment 1 first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick check: confirm the table exists and preview the first few rows\n",
        "df = spark.table(\"main.default.assignment_file\")\n",
        "print(f\"Row count: {df.count()}\")\n",
        "print(f\"Columns: {df.columns}\")\n",
        "df.printSchema()\n",
        "display(df.limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Agent-assisted EDA *(Required)*\n",
        "\n",
        "Use the Data Science Agent to explore the dataset. Below are **sample prompts** to get you started — you don't need to use all of them, and you're encouraged to ask your own follow-up questions. The agent will generate code cells and outputs directly in this notebook.\n",
        "\n",
        "### Sample prompts to try\n",
        "\n",
        "**Data overview and quality:**\n",
        "- \"Describe the `@main.default.assignment_file` dataset. Show column statistics (counts, nulls, unique values) and data types. Think like a data scientist.\"\n",
        "- \"Check for missing values in every column of `@main.default.assignment_file`. Show a summary table and a heatmap of nulls.\"\n",
        "- \"How many duplicate rows are in `@main.default.assignment_file`? Show me examples if any exist.\"\n",
        "\n",
        "**Distribution and patterns:**\n",
        "- \"What are the unique values in the `source` column of `@main.default.assignment_file`? Show a bar chart of how many rows come from each source.\"\n",
        "- \"Compare the average length (in characters) of the `chosen` vs `rejected` responses in `@main.default.assignment_file`. Visualize the distributions.\"\n",
        "- \"Which models appear most frequently as `chosen-model` and `rejected-model`? Show the top 10 of each.\"\n",
        "\n",
        "**Content and usefulness for agent integration:**\n",
        "- \"Show me 5 example rows from `@main.default.assignment_file` where the `source` is 'evol_instruct'. What kind of instructions are these?\"\n",
        "- \"Is there a relationship between the `source` column and which model was chosen? Create a cross-tabulation.\"\n",
        "- \"Which columns contain structured data (e.g., model names, sources) that could be turned into lookup or filtering tools for an agent?\"\n",
        "- \"Identify any columns that would need transformation before being used in an agent workflow (e.g., text that needs parsing, inconsistent formats, columns that aren't useful).\"\n",
        "\n",
        "**Go deeper (optional):**\n",
        "- \"Create a word cloud of the most common terms in the instruction/prompt column.\"\n",
        "- \"Are there any outliers in response length? Show a box plot.\"\n",
        "- \"Summarize the key characteristics of this dataset in a new markdown cell.\"\n",
        "\n",
        "> **Tip:** After the agent generates cells, read the output carefully. If something looks wrong or surprising, ask the agent to investigate further. This back-and-forth is how real agent-assisted analysis works.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*The Data Science Agent will insert cells below as you interact with it. Leave this section open for agent-generated content.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Your analysis *(Required)*\n",
        "\n",
        "After the agent has helped you explore the data, write your own analysis in the sections below. **This is the core deliverable** — the agent does the mechanical work; you provide the thinking.\n",
        "\n",
        "### 4a. What does this dataset contain?\n",
        "\n",
        "Summarize what you found. Include:\n",
        "- How many rows and columns are there?\n",
        "- What do the key columns represent (e.g., source, instruction, chosen, rejected, models)?\n",
        "- What is the overall structure of a single record — what does one row \"mean\"?\n",
        "- Any data quality issues (nulls, duplicates, inconsistencies)?\n",
        "\n",
        "*Write your summary below (replace this text):*\n",
        "\n",
        "**[Your answer here]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4b. How could this data be integrated into an agent?\n",
        "\n",
        "In this course you won't be training models directly — instead, you'll be building **agents** that leverage data through **tools** (Unity Catalog functions, Vector Search, MCP). Based on your EDA, assess how this dataset could be useful in an agent workflow:\n",
        "- Which columns could an agent **query as a tool** (e.g., a function that looks up model preferences by source, or retrieves example prompts by category)?\n",
        "- Could any text columns (instructions, chosen/rejected responses) be indexed for **Vector Search** so an agent can find similar examples?\n",
        "- What parts of the data are useful for **evaluating** an agent's outputs (e.g., using chosen vs rejected as a benchmark for quality)?\n",
        "- Are there columns that are metadata-only and wouldn't be useful in an agent context?\n",
        "- Is the data balanced (e.g., even distribution of sources, models) or skewed? How might that affect an agent tool built on this data?\n",
        "\n",
        "*Write your assessment below (replace this text):*\n",
        "\n",
        "**[Your answer here]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4c. What transformations would you recommend?\n",
        "\n",
        "If you were preparing this data to be used by an agent (e.g., as a queryable tool, a Vector Search index, or an evaluation dataset), what changes would you make?\n",
        "- Do any text columns need parsing, cleaning, or reformatting before an agent could use them?\n",
        "- Would you filter out certain rows (e.g., specific sources, very short/long responses) to improve tool quality?\n",
        "- Would you create new derived columns (e.g., response length difference, model win rate) that an agent could use as features or filters?\n",
        "- Would you split the data into separate tables for different agent tools (e.g., one for lookup, one for Vector Search)?\n",
        "- Any other preprocessing steps?\n",
        "\n",
        "*Write your recommendations below (replace this text):*\n",
        "\n",
        "**[Your answer here]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4d. How well did the Data Science Agent perform?\n",
        "\n",
        "Reflect on using the agent for EDA:\n",
        "- What did the agent do well? Where did it save you time?\n",
        "- Did the agent make any mistakes or produce anything you had to correct?\n",
        "- What prompts worked best? What would you do differently next time?\n",
        "- How does this relate to the idea of \"human-in-the-loop\" that we discussed in Week 1?\n",
        "\n",
        "*Write your reflection below (replace this text):*\n",
        "\n",
        "**[Your answer here]**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Lab complete\n",
        "\n",
        "**Required:**\n",
        "- **Section 2:** The dataset verification cell ran and confirmed the table exists with rows and schema.\n",
        "- **Section 3:** You used the Data Science Agent to explore the data. Agent-generated cells with outputs are visible in the notebook.\n",
        "- **Section 4a:** You wrote a summary of what the dataset contains.\n",
        "- **Section 4b:** You assessed how this data could be integrated into an agent (tools, Vector Search, evaluation).\n",
        "- **Section 4c:** You recommended transformations to prepare the data for agent use.\n",
        "- **Section 4d:** You reflected on the agent's performance and the human-in-the-loop experience.\n",
        "\n",
        "**Submit:** Your executed notebook (`.ipynb` with all outputs, including agent-generated cells) and the completed `SUBMISSION_2.md`.\n",
        "\n",
        "*Next week you'll create tools (Unity Catalog functions, Vector Search, MCP) for the agent you'll build over Weeks 3–5.*\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "4"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "AAI_594_Assignment_2_Lab",
      "widgets": {}
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
