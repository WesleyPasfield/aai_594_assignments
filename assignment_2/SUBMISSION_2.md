# AAI 594 — Assignment 2 Submission

**Name:**  
**Date:**

> **Setup:** Create a `screenshots/` folder in the same directory as this file. Save your screenshot images there and reference them by filename. Your submission folder should look like:
>
> ```
> assignment_2/
> ├── AAI_594_Assignment_2_Lab.ipynb   ← executed notebook with ALL outputs (including agent-generated cells)
> ├── SUBMISSION_2.md                  ← this file
> └── screenshots/
>     └── (any additional screenshots, if needed)
> ```

---

## Deliverables

### 1. Executed Notebook

Submit your executed notebook (`.ipynb` with cell outputs) alongside this file. The notebook should include:

- [ ] **Section 2:** Dataset verification cell ran successfully (row count, columns, schema).
- [ ] **Section 3:** Agent-generated cells are visible with outputs (plots, tables, statistics). You interacted with the Data Science Agent using multiple prompts.
- [ ] **Section 4a:** Written summary of what the dataset contains.
- [ ] **Section 4b:** Written assessment of how this data could be integrated into an agent (tools, Vector Search, evaluation).
- [ ] **Section 4c:** Written transformation recommendations for preparing data for agent use.
- [ ] **Section 4d:** Written reflection on the agent's performance and human-in-the-loop experience.

---

## Grading Rubric

### Criterion 1: Agent-Assisted EDA Execution (25%)

*Did the student effectively use the Data Science Agent to explore the dataset?*

| Meets or Exceeds Expectations | Approaches Expectations | Below Expectations | Inadequate Attempt |
|---|---|---|---|
| Student used the Data Science Agent with multiple, well-crafted prompts covering data overview, distributions, quality, and content analysis. Agent-generated cells show a thorough exploration with diverse visualizations and statistics. Evidence of iterative prompting (follow-up questions, corrections). | Student used the Data Science Agent with several prompts covering at least data overview and distributions. Agent-generated cells show reasonable exploration with some visualizations. | Student used the Data Science Agent minimally (1–2 basic prompts). Exploration is shallow — missing key aspects like data quality, distributions, or content analysis. | Little to no evidence of using the Data Science Agent. Notebook contains only the starter cells or trivial agent output. |

### Criterion 2: Data Assessment and Agent Integration Analysis (35%) - NO AI Usage

*Did the student provide a thoughtful, accurate analysis of the dataset and how it could be integrated into an agent?*

| Meets or Exceeds Expectations | Approaches Expectations | Below Expectations | Inadequate Attempt |
|---|---|---|---|
| Sections 4a–4c are detailed and demonstrate genuine understanding. The student accurately describes the dataset structure, identifies specific ways the data could be used by an agent (e.g., queryable tools, Vector Search indexing, evaluation benchmarks), and provides actionable transformation recommendations grounded in what the EDA revealed. Analysis goes beyond restating agent output — the student adds their own reasoning and connections to course material (e.g., tools, UC functions, agent workflows). | Sections 4a–4c are present and mostly accurate. The student describes the dataset and mentions agent integration, but the analysis is surface-level or restates agent output without adding interpretation. Transformation recommendations are generic (e.g., "clean the data") rather than specific to agent use. | Sections 4a–4c are incomplete or contain significant inaccuracies. The student may confuse column purposes or miss key aspects of how the data could serve an agent. Transformation recommendations are vague or missing. | Sections 4a–4c are missing or contain only placeholder text. No meaningful analysis provided. |

### Criterion 3: Critical Reflection on Agent Performance (20%) - NO AI usage

*Did the student critically evaluate the Data Science Agent as a tool?*

| Meets or Exceeds Expectations | Approaches Expectations | Below Expectations | Inadequate Attempt |
|---|---|---|---|
| Section 4d shows genuine reflection. Student identifies specific strengths and weaknesses of the agent, cites concrete examples from their session (e.g., "the agent misread column X" or "this prompt produced better results than that one"). Connects the experience to human-in-the-loop concepts from Week 1 readings. | Section 4d is present and identifies some strengths/weaknesses, but lacks specific examples or connection to course concepts. | Section 4d is brief or generic (e.g., "the agent was helpful"). No specific examples or critical evaluation. | Section 4d is missing or contains only placeholder text. |

### Criterion 4: Notebook Quality and Organization (20%)

*Is the submitted notebook well-organized, complete, and reproducible?*

| Meets or Exceeds Expectations | Approaches Expectations | Below Expectations | Inadequate Attempt |
|---|---|---|---|
| Notebook is complete with all outputs visible. Agent-generated cells are clearly labeled or separated from student-written analysis. Markdown cells are well-written, organized, and easy to follow. The notebook tells a coherent story from exploration to conclusions. | Notebook has most outputs visible. Organization is adequate but may mix agent output with student analysis in a confusing way. Some markdown cells could be clearer. | Notebook is missing significant outputs or is disorganized. Hard to distinguish between agent-generated content and student analysis. | Notebook is incomplete, missing outputs, or submitted without execution. |

---

## Notes (optional)

Add any notes, issues you ran into, or things you tried here.
