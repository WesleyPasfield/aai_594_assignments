{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAI 594 — Assignment 4a\n",
    "\n",
    "## From Tools to a Working Agent\n",
    "\n",
    "**In this lab you will:**\n",
    "- **Required (Sections 1–6):** Prototype an agent in the **AI Playground**, export the code, and run it in this notebook.\n",
    "- **Required (Section 7):** Register your agent's system prompt in the **Unity Catalog Prompt Registry** and version it.\n",
    "- **Required (Section 8):** Swap the underlying LLM and compare output quality and token usage across models.\n",
    "- **Required (Section 9):** Clean up Vector Search resources.\n",
    "- **Optional, strongly encouraged (Section 10):** Explore **prompt optimization** concepts from the Week 4 readings.\n",
    "\n",
    "### The big picture\n",
    "\n",
    "| Week | What you do | Deliverable |\n",
    "|------|------------|-------------|\n",
    "| 3 | Build tools: UC functions, Vector Search, MCP | Tested tools + MCP config |\n",
    "| **4 (this week)** | **Wire tools into an agent; register a prompt; compare LLMs** | **Working agent** |\n",
    "| 5 | Evaluate the agent with judges and an eval dataset | Evaluation report |\n",
    "\n",
    "**Readings this week:**\n",
    "- [GEPA DSPy Optimization](https://arxiv.org/pdf/2507.19457)\n",
    "- [Everything is Context](https://www.arxiv.org/pdf/2512.05470)\n",
    "- Optional: [Evolving Excellence](https://arxiv.org/html/2512.09108v1)\n",
    "\n",
    "**Key docs:**\n",
    "- [Prototype tool-calling agents in AI Playground](https://docs.databricks.com/aws/en/generative-ai/agent-framework/ai-playground-agent)\n",
    "- [Prompt Registry](https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/)\n",
    "- [Author an agent](https://docs.databricks.com/aws/en/generative-ai/agent-framework/author-agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. From tools to agent *(Required)*\n",
    "\n",
    "In Assignment 3 you created three kinds of tools:\n",
    "\n",
    "| Tool | What it does |\n",
    "|------|--------------|\n",
    "| `main.default.lookup_source_info` | SQL lookup — row count and sample for a source |\n",
    "| `main.default.analyze_instruction` | Python — instruction complexity metrics |\n",
    "| Your custom function | SQL or Python — your own design |\n",
    "| Vector Search index | Semantic search over 1,000 UltraFeedback instructions |\n",
    "| You.com MCP | Live web search (configured in Cursor) |\n",
    "\n",
    "This week you'll wire those tools into a **working agent** — an LLM that can decide which tool to call based on the user's question. The workflow is:\n",
    "\n",
    "1. **Prototype** in the AI Playground (no code)\n",
    "2. **Export** the agent code to a notebook\n",
    "3. **Register** the system prompt in Unity Catalog for versioning\n",
    "4. **Swap LLMs** and compare how different models use the same tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Install dependencies *(Required)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade \"mlflow[databricks]>=3.1.0\" databricks-langchain unitycatalog-ai[databricks] databricks-vectorsearch\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Recreate Vector Search *(Required)*\n",
    "\n",
    "You deleted the Vector Search endpoint and index at the end of Assignment 3 (good resource management). Recreate them now — the agent needs the index to answer similarity questions.\n",
    "\n",
    "This code is the same as Assignment 3, Section 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Check if the VS source table already exists; create if not\n",
    "try:\n",
    "    spark.table(\"main.default.ultrafeedback_vs_source\").limit(1)\n",
    "    print(\"VS source table already exists.\")\n",
    "except:\n",
    "    print(\"Creating VS source table...\")\n",
    "    vs_source = (\n",
    "        spark.table(\"main.default.assignment_file\")\n",
    "        .select(\"source\", \"instruction\")\n",
    "        .dropDuplicates([\"instruction\"])\n",
    "        .limit(1000)\n",
    "        .withColumn(\"id\", monotonically_increasing_id())\n",
    "    )\n",
    "    vs_source.write.format(\"delta\") \\\n",
    "        .option(\"delta.enableChangeDataFeed\", \"true\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"main.default.ultrafeedback_vs_source\")\n",
    "    print(\"VS source table created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "vs_client = VectorSearchClient()\n",
    "VS_ENDPOINT_NAME = \"aai594_vs_endpoint\"\n",
    "VS_INDEX_NAME = \"main.default.ultrafeedback_vs_index\"\n",
    "\n",
    "# Recreate endpoint\n",
    "try:\n",
    "    vs_client.create_endpoint_and_wait(name=VS_ENDPOINT_NAME, endpoint_type=\"STANDARD\")\n",
    "    print(f\"Endpoint '{VS_ENDPOINT_NAME}' is ready.\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"Endpoint '{VS_ENDPOINT_NAME}' already exists — reusing.\")\n",
    "    else:\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate delta sync index\n",
    "try:\n",
    "    index = vs_client.create_delta_sync_index_and_wait(\n",
    "        endpoint_name=VS_ENDPOINT_NAME,\n",
    "        source_table_name=\"main.default.ultrafeedback_vs_source\",\n",
    "        index_name=VS_INDEX_NAME,\n",
    "        pipeline_type=\"TRIGGERED\",\n",
    "        primary_key=\"id\",\n",
    "        embedding_source_column=\"instruction\",\n",
    "        embedding_model_endpoint_name=\"databricks-gte-large-en\"\n",
    "    )\n",
    "    print(f\"Index '{VS_INDEX_NAME}' created and synced.\")\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e).lower():\n",
    "        print(f\"Index '{VS_INDEX_NAME}' already exists — reusing.\")\n",
    "        index = vs_client.get_index(endpoint_name=VS_ENDPOINT_NAME, index_name=VS_INDEX_NAME)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Quick test\n",
    "results = index.similarity_search(query_text=\"machine learning\", columns=[\"id\", \"instruction\"], num_results=2)\n",
    "print(\"VS index is working:\", len(results.get(\"result\", {}).get(\"data_array\", [])), \"results returned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Prototype in the AI Playground *(Required)*\n",
    "\n",
    "The **AI Playground** lets you prototype a tool-calling agent with no code. You select an LLM, attach tools, and chat — then export the working agent as a Python notebook.\n",
    "\n",
    "### Step-by-step\n",
    "\n",
    "1. **Open AI Playground** — In the Databricks sidebar, click **Playground** (under Machine Learning or the top-level menu).\n",
    "\n",
    "2. **Select a Tools-enabled LLM** — In the model dropdown, choose a model that supports tool calling. Good options:\n",
    "   - `databricks-meta-llama-3-3-70b-instruct`\n",
    "   - `databricks-claude-sonnet-4` (if available)\n",
    "   \n",
    "   Make sure the model shows as **\"Tools enabled\"** in the Playground UI.\n",
    "\n",
    "3. **Add your tools** — Click the **Tools** button and add:\n",
    "   - **UC Functions:** `main.default.lookup_source_info`, `main.default.analyze_instruction`, and your custom function from Assignment 3\n",
    "   - **Vector Search index:** `main.default.ultrafeedback_vs_index`\n",
    "   - You can add up to 20 tools total\n",
    "\n",
    "4. **Set a system prompt** — In the System Prompt field, enter something like:\n",
    "\n",
    "   ```\n",
    "   You are the UltraFeedback Expert, an AI assistant that helps users\n",
    "   explore and understand the UltraFeedback LLM preference dataset.\n",
    "   \n",
    "   Use your tools to answer questions accurately:\n",
    "   - Use lookup_source_info to get statistics about data sources\n",
    "   - Use analyze_instruction to assess instruction complexity\n",
    "   - Use Vector Search to find similar instructions by meaning\n",
    "   \n",
    "   Always cite which tool you used and explain the results.\n",
    "   If you don't have a relevant tool, say so rather than guessing.\n",
    "   ```\n",
    "\n",
    "5. **Test the agent** — Try these queries to verify tool calling works:\n",
    "   - *\"How many rows come from the evol_instruct source?\"* (should call `lookup_source_info`)\n",
    "   - *\"Find instructions similar to 'Explain quantum computing'\"* (should call Vector Search)\n",
    "   - *\"Analyze the complexity of: Write a detailed essay about climate change including economic impacts\"* (should call `analyze_instruction`)\n",
    "\n",
    "6. **Export the code** — Once your agent is working:\n",
    "   - Click **Get code** (or **Export**) in the top-right of the Playground\n",
    "   - Select **Create agent notebook**\n",
    "   - Review the generated code\n",
    "\n",
    "> **Take a screenshot** of your agent in the AI Playground with tools attached and a successful tool-calling conversation. Save as `screenshots/ai_playground.png`.\n",
    "\n",
    "**Docs:** [Prototype tool-calling agents in AI Playground](https://docs.databricks.com/aws/en/generative-ai/agent-framework/ai-playground-agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Adapt and run the exported agent code *(Required)*\n",
    "\n",
    "Paste the exported code from the AI Playground into the cells below and run it. If the Playground's export didn't work or you prefer to build manually, use the template provided.\n",
    "\n",
    "The key components are:\n",
    "1. **LLM endpoint** — which Foundation Model to use\n",
    "2. **Tools** — your UC functions wrapped in `UCFunctionToolkit`\n",
    "3. **System prompt** — loaded from the Prompt Registry (you'll register it in the next section)\n",
    "4. **Agent executor** — the LangChain agent that orchestrates LLM + tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from databricks_langchain import ChatDatabricks, UCFunctionToolkit\n",
    "\n",
    "# Enable MLflow tracing so all agent calls are logged\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# ---- 1. Choose the LLM ----\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT, temperature=0.1)\n",
    "\n",
    "# ---- 2. Load tools from Unity Catalog ----\n",
    "# Add your UC function names here (include your custom function from Assignment 3)\n",
    "UC_FUNCTION_NAMES = [\n",
    "    \"main.default.lookup_source_info\",\n",
    "    \"main.default.analyze_instruction\",\n",
    "    # \"main.default.your_custom_function\",  # <-- uncomment and replace with your function\n",
    "]\n",
    "\n",
    "toolkit = UCFunctionToolkit(function_names=UC_FUNCTION_NAMES)\n",
    "tools = toolkit.tools\n",
    "print(f\"Loaded {len(tools)} UC function tools: {[t.name for t in tools]}\")\n",
    "\n",
    "# ---- 3. Define the system prompt ----\n",
    "SYSTEM_PROMPT = \"\"\"You are the UltraFeedback Expert, an AI assistant that helps users\n",
    "explore and understand the UltraFeedback LLM preference dataset.\n",
    "\n",
    "Use your tools to answer questions accurately:\n",
    "- Use lookup_source_info to get statistics about data sources\n",
    "- Use analyze_instruction to assess instruction complexity\n",
    "\n",
    "Always cite which tool you used and explain the results.\n",
    "If you don't have a relevant tool, say so rather than guessing.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# ---- 4. Create and run the agent ----\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "print(\"Agent ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a query that should trigger a tool call\n",
    "response = agent_executor.invoke({\"input\": \"How many rows come from the evol_instruct source?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a complexity analysis query\n",
    "response2 = agent_executor.invoke({\n",
    "    \"input\": \"Analyze the complexity of this instruction: Explain the process of photosynthesis in detail, including light-dependent and light-independent reactions.\"\n",
    "})\n",
    "print(response2[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a general knowledge question (should say it doesn't have a tool)\n",
    "response3 = agent_executor.invoke({\"input\": \"What is the capital of France?\"})\n",
    "print(response3[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Verify UC functions are registered *(Required)*\n",
    "\n",
    "Quick check that all your tools are still in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SHOW USER FUNCTIONS IN main.default;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Register a prompt in Unity Catalog *(Required)*\n",
    "\n",
    "The **MLflow Prompt Registry** lets you version and manage prompt templates in Unity Catalog. This is important because:\n",
    "\n",
    "- **Versioning:** Every change creates an immutable snapshot. You can roll back if a new prompt performs worse.\n",
    "- **Aliases:** Point \"production\" at a specific version. Update the alias without changing code.\n",
    "- **Collaboration:** Non-engineers can edit prompts through the UI.\n",
    "- **Governance:** Unity Catalog tracks who changed what and when.\n",
    "\n",
    "**Docs:** [Prompt Registry](https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/) · [Create and edit prompts](https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/create-and-edit-prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Register the system prompt as a versioned prompt in Unity Catalog\n",
    "PROMPT_NAME = \"main.default.ultrafeedback_expert_prompt\"\n",
    "\n",
    "prompt_info = mlflow.genai.register_prompt(\n",
    "    name=PROMPT_NAME,\n",
    "    template=SYSTEM_PROMPT,\n",
    "    commit_message=\"Initial system prompt for the UltraFeedback Expert agent\"\n",
    ")\n",
    "\n",
    "print(f\"Registered: {prompt_info.name}, version: {prompt_info.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an alias so we can reference \"production\" without knowing the version number\n",
    "mlflow.genai.set_prompt_alias(\n",
    "    name=PROMPT_NAME,\n",
    "    alias=\"production\",\n",
    "    version=prompt_info.version\n",
    ")\n",
    "print(f\"Alias 'production' set to version {prompt_info.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate loading the prompt by alias — this is how your agent would\n",
    "# load the prompt in production (decoupled from the version number)\n",
    "loaded = mlflow.genai.load_prompt(f\"{PROMPT_NAME}@production\")\n",
    "print(\"Loaded prompt template:\")\n",
    "print(loaded.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a second version with a small improvement\n",
    "IMPROVED_PROMPT = \"\"\"You are the UltraFeedback Expert, an AI assistant that helps users\n",
    "explore and understand the UltraFeedback LLM preference dataset.\n",
    "\n",
    "Use your tools to answer questions accurately:\n",
    "- Use lookup_source_info to get statistics about data sources\n",
    "- Use analyze_instruction to assess instruction complexity\n",
    "\n",
    "Always cite which tool you used and explain the results.\n",
    "If you don't have a relevant tool, say so rather than guessing.\n",
    "When comparing sources or models, use specific numbers from the data.\"\"\"\n",
    "\n",
    "v2 = mlflow.genai.register_prompt(\n",
    "    name=PROMPT_NAME,\n",
    "    template=IMPROVED_PROMPT,\n",
    "    commit_message=\"Added instruction to cite specific numbers when comparing\"\n",
    ")\n",
    "print(f\"Version {v2.version} registered. You now have two versions.\")\n",
    "print(f\"Alias 'production' still points to version {prompt_info.version} (you can update it after testing).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Swap LLMs and compare *(Required)*\n",
    "\n",
    "A key advantage of the agent architecture is that you can **swap the underlying LLM** without changing the tools or prompt. Different models may:\n",
    "- Call tools more or less reliably\n",
    "- Produce more concise or verbose answers\n",
    "- Use more or fewer tokens\n",
    "\n",
    "You'll run the **same three test queries** against two different LLMs and compare the results.\n",
    "\n",
    "### Test queries\n",
    "1. *\"How many rows come from the sharegpt source?\"*\n",
    "2. *\"Analyze the complexity of: What is 2+2?\"*\n",
    "3. *\"What sources are available in the dataset and how do they compare in size?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run test queries and collect results\n",
    "TEST_QUERIES = [\n",
    "    \"How many rows come from the sharegpt source?\",\n",
    "    \"Analyze the complexity of: What is 2+2?\",\n",
    "    \"What sources are available in the dataset and how do they compare in size?\",\n",
    "]\n",
    "\n",
    "def run_test_queries(executor, model_name):\n",
    "    \"\"\"Run test queries and return results with the model name.\"\"\"\n",
    "    results = []\n",
    "    for q in TEST_QUERIES:\n",
    "        print(f\"\\n--- {model_name} | Query: {q[:60]}... ---\")\n",
    "        resp = executor.invoke({\"input\": q})\n",
    "        output = resp[\"output\"]\n",
    "        print(output[:300])\n",
    "        results.append({\"model\": model_name, \"query\": q, \"output\": output})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Model A ----\n",
    "MODEL_A = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm_a = ChatDatabricks(endpoint=MODEL_A, temperature=0.1)\n",
    "agent_a = create_tool_calling_agent(llm_a, tools, prompt)\n",
    "executor_a = AgentExecutor(agent=agent_a, tools=tools, verbose=False)\n",
    "\n",
    "print(f\"=== Testing Model A: {MODEL_A} ===\")\n",
    "results_a = run_test_queries(executor_a, MODEL_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Model B ----\n",
    "# Change this to another available Foundation Model endpoint.\n",
    "# Check which models are available in your workspace under Serving > Foundation Models.\n",
    "# Options may include: databricks-claude-sonnet-4, databricks-dbrx-instruct, etc.\n",
    "MODEL_B = \"databricks-claude-sonnet-4\"  # <-- change if this model is not available\n",
    "\n",
    "try:\n",
    "    llm_b = ChatDatabricks(endpoint=MODEL_B, temperature=0.1)\n",
    "    agent_b = create_tool_calling_agent(llm_b, tools, prompt)\n",
    "    executor_b = AgentExecutor(agent=agent_b, tools=tools, verbose=False)\n",
    "\n",
    "    print(f\"=== Testing Model B: {MODEL_B} ===\")\n",
    "    results_b = run_test_queries(executor_b, MODEL_B)\n",
    "except Exception as e:\n",
    "    print(f\"Model B ({MODEL_B}) not available: {e}\")\n",
    "    print(\"Try a different endpoint name. Check Serving > Foundation Models in the sidebar.\")\n",
    "    results_b = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comparison analysis\n",
    "\n",
    "Compare the two models across the three test queries. Consider:\n",
    "- **Tool usage:** Did both models call the right tools? Did one make unnecessary calls?\n",
    "- **Output quality:** Which responses were more accurate, specific, or helpful?\n",
    "- **Verbosity:** Which model was more concise? Is that better or worse for this use case?\n",
    "- **Error handling:** Did either model hallucinate or fail to use a tool when it should have?\n",
    "\n",
    "*Write your analysis below (replace this text):*\n",
    "\n",
    "**[Your comparison analysis here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Clean up *(Required)*\n",
    "\n",
    "Delete the Vector Search endpoint and index to conserve Free Edition resources. You'll recreate them in Assignment 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete index first, then endpoint\n",
    "try:\n",
    "    vs_client.delete_index(endpoint_name=VS_ENDPOINT_NAME, index_name=VS_INDEX_NAME)\n",
    "    print(f\"Index '{VS_INDEX_NAME}' deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Index deletion note: {e}\")\n",
    "\n",
    "try:\n",
    "    vs_client.delete_endpoint(name=VS_ENDPOINT_NAME)\n",
    "    print(f\"Endpoint '{VS_ENDPOINT_NAME}' deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Endpoint deletion note: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Prompt optimization *(Optional, strongly encouraged)*\n",
    "\n",
    "The Week 4 readings cover **prompt optimization** — using algorithms to systematically improve prompts rather than relying on manual iteration.\n",
    "\n",
    "### Key concepts from the readings\n",
    "\n",
    "- **DSPy** treats prompts as programs with optimizable parameters. Instead of hand-crafting prompts, you define a task signature and let an optimizer (like MIPRO or BootstrapFewShot) find better instructions and examples.\n",
    "- **\"Everything is Context\"** argues that prompts, tools, and memory are all forms of context — optimizing one often helps the others.\n",
    "- **GEPA** extends DSPy optimization to multi-step agentic workflows.\n",
    "\n",
    "### Reflection\n",
    "\n",
    "Based on your experience comparing two LLMs in Section 8 and the readings, answer:\n",
    "\n",
    "1. How could prompt optimization improve your UltraFeedback Expert agent? What would you optimize?\n",
    "2. Why might automated prompt optimization be more effective than manual prompt engineering for complex agents?\n",
    "3. If you had to set up a DSPy optimization for this agent, what would your metric (evaluation function) look like?\n",
    "\n",
    "*Write your reflection below (replace this text):*\n",
    "\n",
    "**[Your reflection here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab complete\n",
    "\n",
    "### Required (Sections 1–9)\n",
    "- [ ] **Section 3:** Vector Search endpoint and index recreated and verified.\n",
    "- [ ] **Section 4:** Prototyped the agent in AI Playground with tools attached (screenshot taken).\n",
    "- [ ] **Section 5:** Exported or adapted agent code runs in this notebook. Agent answers test queries using tools.\n",
    "- [ ] **Section 6:** UC functions verified.\n",
    "- [ ] **Section 7:** System prompt registered in Unity Catalog Prompt Registry with two versions and a \"production\" alias.\n",
    "- [ ] **Section 8:** Ran the same 3 queries against 2 different LLMs. Written comparison analysis provided.\n",
    "- [ ] **Section 9:** Vector Search endpoint and index deleted.\n",
    "\n",
    "### Optional but strongly encouraged (Section 10)\n",
    "- [ ] **Section 10:** Written reflection on prompt optimization.\n",
    "\n",
    "**Also submit:** `PROPOSAL_4b.md` — your final project proposal.\n",
    "\n",
    "**Submit:** Your executed notebook (`.ipynb` with all outputs), `SUBMISSION_4a.md`, and `PROPOSAL_4b.md`.\n",
    "\n",
    "*Next week you'll evaluate the agent using built-in judges, guidelines judges, and custom judges.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}