{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean up\n",
    "\n",
    "No Vector Search resources to clean up \u2014 the embeddings are stored in a Delta table (`main.default.ultrafeedback_embeddings`) which persists across assignments at no additional cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. From tools to agent *(Required)*\n",
    "\n",
    "In Assignment 3 you created three kinds of tools:\n",
    "\n",
    "| Tool | What it does |\n",
    "|------|--------------|\n",
    "| `main.default.lookup_source_info` | SQL lookup \u2014 row count and sample for a source |\n",
    "| `main.default.analyze_instruction` | Python \u2014 instruction complexity metrics |\n",
    "| Your custom function | SQL or Python \u2014 your own design |\n",
    "| Vector Search index | Semantic search over 1,000 UltraFeedback instructions |\n",
    "| You.com MCP | Live web search (configured in Cursor) |\n",
    "\n",
    "This week you'll wire those tools into a **working agent** \u2014 an LLM that can decide which tool to call based on the user's question. The workflow is:\n",
    "\n",
    "1. **Prototype** in the AI Playground (no code)\n",
    "2. **Export** the agent code to a notebook\n",
    "3. **Register** the system prompt in Unity Catalog for versioning\n",
    "4. **Swap LLMs** and compare how different models use the same tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Install dependencies *(Required)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade \"mlflow[databricks]>=3.1.0\" databricks-langchain unitycatalog-ai[databricks] numpy\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Verify embeddings table *(Required)*\n",
    "\n",
    "The embeddings table (`main.default.ultrafeedback_embeddings`) was created in Assignment 3. Verify it exists and has the expected data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the embeddings table from Assignment 3 exists\n",
    "emb_df = spark.table(\"main.default.ultrafeedback_embeddings\")\n",
    "print(f\"Embeddings table has {emb_df.count()} rows.\")\n",
    "print(f\"Columns: {emb_df.columns}\")\n",
    "display(emb_df.select(\"id\", \"instruction\", \"source\").limit(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Prototype in the AI Playground *(Required)*\n",
    "\n",
    "The **AI Playground** lets you prototype a tool-calling agent with no code. You select an LLM, attach tools, and chat \u2014 then export the working agent as a Python notebook.\n",
    "\n",
    "### Step-by-step\n",
    "\n",
    "1. **Open AI Playground** \u2014 In the Databricks sidebar, click **Playground** (under Machine Learning or the top-level menu).\n",
    "\n",
    "2. **Select a Tools-enabled LLM** \u2014 In the model dropdown, choose a model that supports tool calling. Good options:\n",
    "   - `databricks-meta-llama-3-3-70b-instruct`\n",
    "   - `databricks-claude-sonnet-4` (if available)\n",
    "   \n",
    "   Make sure the model shows as **\"Tools enabled\"** in the Playground UI.\n",
    "\n",
    "3. **Add your tools** \u2014 Click the **Tools** button and add:\n",
    "   - **UC Functions:** `main.default.lookup_source_info`, `main.default.analyze_instruction`, and your custom function from Assignment 3\n",
    "   - **UC Functions (semantic search):** `main.default.search_similar_instructions` (embedding-based search from Assignment 3)\n",
    "   - You can add up to 20 tools total\n",
    "\n",
    "4. **Set a system prompt** \u2014 In the System Prompt field, enter something like:\n",
    "\n",
    "   ```\n",
    "   You are the UltraFeedback Expert, an AI assistant that helps users\n",
    "   explore and understand the UltraFeedback LLM preference dataset.\n",
    "   \n",
    "   Use your tools to answer questions accurately:\n",
    "   - Use lookup_source_info to get statistics about data sources\n",
    "   - Use analyze_instruction to assess instruction complexity\n",
    "   - Use search_similar_instructions to find similar instructions by meaning\n",
    "   \n",
    "   Always cite which tool you used and explain the results.\n",
    "   If you don't have a relevant tool, say so rather than guessing.\n",
    "   ```\n",
    "\n",
    "5. **Test the agent** \u2014 Try these queries to verify tool calling works:\n",
    "   - *\"How many rows come from the evol_instruct source?\"* (should call `lookup_source_info`)\n",
    "   - *\"Find instructions similar to 'Explain quantum computing'\"* (should call Vector Search)\n",
    "   - *\"Analyze the complexity of: Write a detailed essay about climate change including economic impacts\"* (should call `analyze_instruction`)\n",
    "\n",
    "6. **Export the code** \u2014 Once your agent is working:\n",
    "   - Click **Get code** (or **Export**) in the top-right of the Playground\n",
    "   - Select **Create agent notebook**\n",
    "   - Review the generated code\n",
    "\n",
    "> **Take a screenshot** of your agent in the AI Playground with tools attached and a successful tool-calling conversation. Save as `screenshots/ai_playground.png`.\n",
    "\n",
    "**Docs:** [Prototype tool-calling agents in AI Playground](https://docs.databricks.com/aws/en/generative-ai/agent-framework/ai-playground-agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Adapt and run the exported agent code *(Required)*\n",
    "\n",
    "Paste the exported code from the AI Playground into the cells below and run it. If the Playground's export didn't work or you prefer to build manually, use the template provided.\n",
    "\n",
    "The key components are:\n",
    "1. **LLM endpoint** \u2014 which Foundation Model to use\n",
    "2. **Tools** \u2014 your UC functions wrapped in `UCFunctionToolkit`\n",
    "3. **System prompt** \u2014 loaded from the Prompt Registry (you'll register it in the next section)\n",
    "4. **Agent executor** \u2014 the LangChain agent that orchestrates LLM + tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from databricks_langchain import ChatDatabricks, UCFunctionToolkit\n",
    "\n",
    "# Enable MLflow tracing so all agent calls are logged\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# ---- 1. Choose the LLM ----\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT, temperature=0.1)\n",
    "\n",
    "# ---- 2. Load tools from Unity Catalog ----\n",
    "# Add your UC function names here (include your custom function from Assignment 3)\n",
    "UC_FUNCTION_NAMES = [\n",
    "    \"main.default.lookup_source_info\",\n",
    "    \"main.default.analyze_instruction\",\n",
    "    # \"main.default.your_custom_function\",  # <-- uncomment and replace with your function\n",
    "]\n",
    "\n",
    "toolkit = UCFunctionToolkit(function_names=UC_FUNCTION_NAMES)\n",
    "tools = toolkit.tools\n",
    "print(f\"Loaded {len(tools)} UC function tools: {[t.name for t in tools]}\")\n",
    "\n",
    "# ---- 3. Define the system prompt ----\n",
    "SYSTEM_PROMPT = \"\"\"You are the UltraFeedback Expert, an AI assistant that helps users\n",
    "explore and understand the UltraFeedback LLM preference dataset.\n",
    "\n",
    "Use your tools to answer questions accurately:\n",
    "- Use lookup_source_info to get statistics about data sources\n",
    "- Use analyze_instruction to assess instruction complexity\n",
    "\n",
    "Always cite which tool you used and explain the results.\n",
    "If you don't have a relevant tool, say so rather than guessing.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "# ---- 4. Create and run the agent ----\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "print(\"Agent ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent with a query that should trigger a tool call\n",
    "response = agent_executor.invoke({\"input\": \"How many rows come from the evol_instruct source?\"})\n",
    "print(response[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a complexity analysis query\n",
    "response2 = agent_executor.invoke({\n",
    "    \"input\": \"Analyze the complexity of this instruction: Explain the process of photosynthesis in detail, including light-dependent and light-independent reactions.\"\n",
    "})\n",
    "print(response2[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a general knowledge question (should say it doesn't have a tool)\n",
    "response3 = agent_executor.invoke({\"input\": \"What is the capital of France?\"})\n",
    "print(response3[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Verify UC functions are registered *(Required)*\n",
    "\n",
    "Quick check that all your tools are still in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SHOW USER FUNCTIONS IN main.default;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Register a prompt in Unity Catalog *(Required)*\n",
    "\n",
    "The **MLflow Prompt Registry** lets you version and manage prompt templates in Unity Catalog. This is important because:\n",
    "\n",
    "- **Versioning:** Every change creates an immutable snapshot. You can roll back if a new prompt performs worse.\n",
    "- **Aliases:** Point \"production\" at a specific version. Update the alias without changing code.\n",
    "- **Collaboration:** Non-engineers can edit prompts through the UI.\n",
    "- **Governance:** Unity Catalog tracks who changed what and when.\n",
    "\n",
    "**Docs:** [Prompt Registry](https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/) \u00b7 [Create and edit prompts](https://docs.databricks.com/aws/en/mlflow3/genai/prompt-version-mgmt/prompt-registry/create-and-edit-prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Register the system prompt as a versioned prompt in Unity Catalog\n",
    "PROMPT_NAME = \"main.default.ultrafeedback_expert_prompt\"\n",
    "\n",
    "prompt_info = mlflow.genai.register_prompt(\n",
    "    name=PROMPT_NAME,\n",
    "    template=SYSTEM_PROMPT,\n",
    "    commit_message=\"Initial system prompt for the UltraFeedback Expert agent\"\n",
    ")\n",
    "\n",
    "print(f\"Registered: {prompt_info.name}, version: {prompt_info.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an alias so we can reference \"production\" without knowing the version number\n",
    "mlflow.genai.set_prompt_alias(\n",
    "    name=PROMPT_NAME,\n",
    "    alias=\"production\",\n",
    "    version=prompt_info.version\n",
    ")\n",
    "print(f\"Alias 'production' set to version {prompt_info.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate loading the prompt by alias \u2014 this is how your agent would\n",
    "# load the prompt in production (decoupled from the version number)\n",
    "loaded = mlflow.genai.load_prompt(f\"{PROMPT_NAME}@production\")\n",
    "print(\"Loaded prompt template:\")\n",
    "print(loaded.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a second version with a small improvement\n",
    "IMPROVED_PROMPT = \"\"\"You are the UltraFeedback Expert, an AI assistant that helps users\n",
    "explore and understand the UltraFeedback LLM preference dataset.\n",
    "\n",
    "Use your tools to answer questions accurately:\n",
    "- Use lookup_source_info to get statistics about data sources\n",
    "- Use analyze_instruction to assess instruction complexity\n",
    "\n",
    "Always cite which tool you used and explain the results.\n",
    "If you don't have a relevant tool, say so rather than guessing.\n",
    "When comparing sources or models, use specific numbers from the data.\"\"\"\n",
    "\n",
    "v2 = mlflow.genai.register_prompt(\n",
    "    name=PROMPT_NAME,\n",
    "    template=IMPROVED_PROMPT,\n",
    "    commit_message=\"Added instruction to cite specific numbers when comparing\"\n",
    ")\n",
    "print(f\"Version {v2.version} registered. You now have two versions.\")\n",
    "print(f\"Alias 'production' still points to version {prompt_info.version} (you can update it after testing).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Swap LLMs and compare *(Required)*\n",
    "\n",
    "A key advantage of the agent architecture is that you can **swap the underlying LLM** without changing the tools or prompt. Different models may:\n",
    "- Call tools more or less reliably\n",
    "- Produce more concise or verbose answers\n",
    "- Use more or fewer tokens\n",
    "\n",
    "You'll run the **same three test queries** against two different LLMs and compare the results.\n",
    "\n",
    "### Test queries\n",
    "1. *\"How many rows come from the sharegpt source?\"*\n",
    "2. *\"Analyze the complexity of: What is 2+2?\"*\n",
    "3. *\"What sources are available in the dataset and how do they compare in size?\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run test queries and collect results\n",
    "TEST_QUERIES = [\n",
    "    \"How many rows come from the sharegpt source?\",\n",
    "    \"Analyze the complexity of: What is 2+2?\",\n",
    "    \"What sources are available in the dataset and how do they compare in size?\",\n",
    "]\n",
    "\n",
    "def run_test_queries(executor, model_name):\n",
    "    \"\"\"Run test queries and return results with the model name.\"\"\"\n",
    "    results = []\n",
    "    for q in TEST_QUERIES:\n",
    "        print(f\"\\n--- {model_name} | Query: {q[:60]}... ---\")\n",
    "        resp = executor.invoke({\"input\": q})\n",
    "        output = resp[\"output\"]\n",
    "        print(output[:300])\n",
    "        results.append({\"model\": model_name, \"query\": q, \"output\": output})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Model A ----\n",
    "MODEL_A = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm_a = ChatDatabricks(endpoint=MODEL_A, temperature=0.1)\n",
    "agent_a = create_tool_calling_agent(llm_a, tools, prompt)\n",
    "executor_a = AgentExecutor(agent=agent_a, tools=tools, verbose=False)\n",
    "\n",
    "print(f\"=== Testing Model A: {MODEL_A} ===\")\n",
    "results_a = run_test_queries(executor_a, MODEL_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Model B ----\n",
    "# Change this to another available Foundation Model endpoint.\n",
    "# Check which models are available in your workspace under Serving > Foundation Models.\n",
    "# Options may include: databricks-claude-sonnet-4, databricks-dbrx-instruct, etc.\n",
    "MODEL_B = \"databricks-claude-sonnet-4\"  # <-- change if this model is not available\n",
    "\n",
    "try:\n",
    "    llm_b = ChatDatabricks(endpoint=MODEL_B, temperature=0.1)\n",
    "    agent_b = create_tool_calling_agent(llm_b, tools, prompt)\n",
    "    executor_b = AgentExecutor(agent=agent_b, tools=tools, verbose=False)\n",
    "\n",
    "    print(f\"=== Testing Model B: {MODEL_B} ===\")\n",
    "    results_b = run_test_queries(executor_b, MODEL_B)\n",
    "except Exception as e:\n",
    "    print(f\"Model B ({MODEL_B}) not available: {e}\")\n",
    "    print(\"Try a different endpoint name. Check Serving > Foundation Models in the sidebar.\")\n",
    "    results_b = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comparison analysis\n",
    "\n",
    "Compare the two models across the three test queries. Consider:\n",
    "- **Tool usage:** Did both models call the right tools? Did one make unnecessary calls?\n",
    "- **Output quality:** Which responses were more accurate, specific, or helpful?\n",
    "- **Verbosity:** Which model was more concise? Is that better or worse for this use case?\n",
    "- **Error handling:** Did either model hallucinate or fail to use a tool when it should have?\n",
    "\n",
    "*Write your analysis below (replace this text):*\n",
    "\n",
    "**[Your comparison analysis here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean up\n",
    "\n",
    "No Vector Search resources to clean up \u2014 the embeddings are stored in a Delta table (`main.default.ultrafeedback_embeddings`) which persists across assignments at no additional cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Prompt optimization *(Optional, strongly encouraged)*\n",
    "\n",
    "The Week 4 readings cover **prompt optimization** \u2014 using algorithms to systematically improve prompts rather than relying on manual iteration.\n",
    "\n",
    "### Key concepts from the readings\n",
    "\n",
    "- **DSPy** treats prompts as programs with optimizable parameters. Instead of hand-crafting prompts, you define a task signature and let an optimizer (like MIPRO or BootstrapFewShot) find better instructions and examples.\n",
    "- **\"Everything is Context\"** argues that prompts, tools, and memory are all forms of context \u2014 optimizing one often helps the others.\n",
    "- **GEPA** extends DSPy optimization to multi-step agentic workflows.\n",
    "\n",
    "### Reflection\n",
    "\n",
    "Based on your experience comparing two LLMs in Section 8 and the readings, answer:\n",
    "\n",
    "1. How could prompt optimization improve your UltraFeedback Expert agent? What would you optimize?\n",
    "2. Why might automated prompt optimization be more effective than manual prompt engineering for complex agents?\n",
    "3. If you had to set up a DSPy optimization for this agent, what would your metric (evaluation function) look like?\n",
    "\n",
    "*Write your reflection below (replace this text):*\n",
    "\n",
    "**[Your reflection here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab complete\n",
    "\n",
    "### Required (Sections 1\u20139)\n",
    "- [ ] **Section 3:** Embeddings table verified.\n",
    "- [ ] **Section 4:** Prototyped the agent in AI Playground with tools attached (screenshot taken).\n",
    "- [ ] **Section 5:** Exported or adapted agent code runs in this notebook. Agent answers test queries using tools.\n",
    "- [ ] **Section 6:** UC functions verified.\n",
    "- [ ] **Section 7:** System prompt registered in Unity Catalog Prompt Registry with two versions and a \"production\" alias.\n",
    "- [ ] **Section 8:** Ran the same 3 queries against 2 different LLMs. Written comparison analysis provided.\n",
    "- [ ] **Section 9:** No VS cleanup needed.\n",
    "\n",
    "### Optional but strongly encouraged (Section 10)\n",
    "- [ ] **Section 10:** Written reflection on prompt optimization.\n",
    "\n",
    "**Also submit:** `PROPOSAL_4b.md` \u2014 your final project proposal.\n",
    "\n",
    "**Submit:** Your executed notebook (`.ipynb` with all outputs), `SUBMISSION_4a.md`, and `PROPOSAL_4b.md`.\n",
    "\n",
    "*Next week you'll evaluate the agent using built-in judges, guidelines judges, and custom judges.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}