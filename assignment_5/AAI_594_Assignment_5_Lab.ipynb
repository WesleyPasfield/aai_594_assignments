{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Clean up\n",
    "\n",
    "No Vector Search resources to clean up \u2014 the embeddings are stored in a Delta table (`main.default.ultrafeedback_embeddings`) which persists across assignments at no additional cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Why evaluate agents? *(Required)*\n",
    "\n",
    "An agent that *seems* to work in a few demos might fail on edge cases, hallucinate tool results, or give confidently wrong answers. **Evaluation** is how you find out before your users do.\n",
    "\n",
    "MLflow 3 provides three types of judges:\n",
    "\n",
    "| Judge type | What it does | When to use |\n",
    "|-----------|-------------|-------------|\n",
    "| **Built-in** (e.g., `Correctness`, `Safety`) | Pre-configured scorers with standard rubrics | Quick baseline \u2014 does the agent give correct, safe answers? |\n",
    "| **Guidelines** | You write natural-language rules; the judge checks compliance | Domain-specific quality bars (e.g., \"must cite the data source\") |\n",
    "| **Custom** (`make_judge()`) | You write the full prompt template | Full control \u2014 your own rubric, scoring, and output format |\n",
    "\n",
    "In this assignment you'll use all three on your UltraFeedback Expert agent and compare how they rate the same outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Install dependencies *(Required)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade \"mlflow[databricks]>=3.1.0\" databricks-langchain unitycatalog-ai[databricks] numpy databricks-agents\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Verify embeddings table *(Required)*\n",
    "\n",
    "The embeddings table (`main.default.ultrafeedback_embeddings`) was created in Assignment 3. Verify it exists and has the expected data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the embeddings table from Assignment 3 exists\n",
    "emb_df = spark.table(\"main.default.ultrafeedback_embeddings\")\n",
    "print(f\"Embeddings table has {emb_df.count()} rows.\")\n",
    "print(f\"Columns: {emb_df.columns}\")\n",
    "display(emb_df.select(\"id\", \"instruction\", \"source\").limit(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "## 3. Verify embeddings table *(Required)*\n",
    "\n",
    "The embeddings table (`main.default.ultrafeedback_embeddings`) was created in Assignment 3. Verify it exists and has the expected data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the embeddings table from Assignment 3 exists\n",
    "emb_df = spark.table(\"main.default.ultrafeedback_embeddings\")\n",
    "print(f\"Embeddings table has {emb_df.count()} rows.\")\n",
    "print(f\"Columns: {emb_df.columns}\")\n",
    "display(emb_df.select(\"id\", \"instruction\", \"source\").limit(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from databricks_langchain import ChatDatabricks, UCFunctionToolkit\n",
    "\n",
    "# Enable tracing\n",
    "mlflow.langchain.autolog()\n",
    "mlflow.set_experiment(\"/Users/\" + spark.sql(\"SELECT current_user()\").first()[0] + \"/aai594_assignment5_eval\")\n",
    "\n",
    "# Load prompt from UC Prompt Registry\n",
    "PROMPT_NAME = \"main.default.ultrafeedback_expert_prompt\"\n",
    "try:\n",
    "    loaded_prompt = mlflow.genai.load_prompt(f\"{PROMPT_NAME}@production\")\n",
    "    SYSTEM_PROMPT = loaded_prompt.template\n",
    "    print(f\"Loaded prompt from registry: {PROMPT_NAME}@production\")\n",
    "except:\n",
    "    # Fallback if prompt wasn't registered in Assignment 4\n",
    "    SYSTEM_PROMPT = \"\"\"You are the UltraFeedback Expert, an AI assistant that helps users\n",
    "explore and understand the UltraFeedback LLM preference dataset.\n",
    "Use your tools to answer questions accurately.\n",
    "Always cite which tool you used and explain the results.\n",
    "If you don't have a relevant tool, say so rather than guessing.\"\"\"\n",
    "    print(\"Using fallback prompt (prompt registry not available).\")\n",
    "\n",
    "# Build the agent\n",
    "LLM_ENDPOINT = \"databricks-meta-llama-3-3-70b-instruct\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT, temperature=0.1)\n",
    "\n",
    "UC_FUNCTION_NAMES = [\n",
    "    \"main.default.lookup_source_info\",\n",
    "    \"main.default.analyze_instruction\",\n",
    "    # \"main.default.your_custom_function\",  # <-- add your custom function\n",
    "]\n",
    "toolkit = UCFunctionToolkit(function_names=UC_FUNCTION_NAMES)\n",
    "tools = toolkit.tools\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
    "\n",
    "# Quick test\n",
    "resp = agent_executor.invoke({\"input\": \"How many rows come from evol_instruct?\"})\n",
    "print(\"Agent test:\", resp[\"output\"][:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Create an evaluation dataset *(Required)*\n",
    "\n",
    "An evaluation dataset is a set of questions (inputs) with optional expected answers (expectations). You need enough variety to test different agent capabilities.\n",
    "\n",
    "We'll create the dataset in two ways:\n",
    "1. **Manually** \u2014 you write questions that specifically target your tools\n",
    "2. **Synthetically** \u2014 use an LLM to generate additional questions\n",
    "\n",
    "**Docs:** [Build evaluation datasets](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/build-eval-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Manual evaluation questions ----\n",
    "# These target specific tool capabilities. Add 5 of your own at the end.\n",
    "manual_eval_data = [\n",
    "    # Should trigger lookup_source_info\n",
    "    {\n",
    "        \"inputs\": {\"input\": \"How many rows come from the sharegpt source?\"},\n",
    "        \"expectations\": {\"expected_facts\": [\"should call lookup_source_info\", \"should return a row count\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"input\": \"What kind of instructions does the flan_v2 source contain?\"},\n",
    "        \"expectations\": {\"expected_facts\": [\"should call lookup_source_info\", \"should show a sample instruction\"]}\n",
    "    },\n",
    "    # Should trigger analyze_instruction\n",
    "    {\n",
    "        \"inputs\": {\"input\": \"Analyze the complexity of: What is 2+2?\"},\n",
    "        \"expectations\": {\"expected_facts\": [\"should call analyze_instruction\", \"should report low complexity\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"input\": \"Analyze this instruction: Write a comprehensive essay on the socioeconomic impacts of climate change across developing nations, including policy recommendations.\"},\n",
    "        \"expectations\": {\"expected_facts\": [\"should call analyze_instruction\", \"should report high complexity\"]}\n",
    "    },\n",
    "    # Should NOT trigger tools (tests the agent's judgment)\n",
    "    {\n",
    "        \"inputs\": {\"input\": \"What is the capital of France?\"},\n",
    "        \"expectations\": {\"expected_facts\": [\"should say it doesn't have a relevant tool or answer from general knowledge\"]}\n",
    "    },\n",
    "    # Multi-step or ambiguous\n",
    "    {\n",
    "        \"inputs\": {\"input\": \"Compare the evol_instruct and sharegpt sources. Which has more data?\"},\n",
    "        \"expectations\": {\"expected_facts\": [\"should call lookup_source_info for both sources\", \"should compare the row counts\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"input\": \"What sources are available in the dataset?\"},\n",
    "        \"expectations\": {\"expected_facts\": [\"should list multiple source names\"]}\n",
    "    },\n",
    "    # Edge cases\n",
    "    {\n",
    "        \"inputs\": {\"input\": \"Look up the source called 'nonexistent_source_xyz'\"},\n",
    "        \"expectations\": {\"expected_facts\": [\"should call lookup_source_info\", \"should indicate no data found or zero rows\"]}\n",
    "    },\n",
    "\n",
    "    # ---- YOUR 5 QUESTIONS BELOW ----\n",
    "    # Add 5 more evaluation questions that test different aspects of the agent.\n",
    "    # Include expected_facts for each.\n",
    "    # {\n",
    "    #     \"inputs\": {\"input\": \"YOUR QUESTION HERE\"},\n",
    "    #     \"expectations\": {\"expected_facts\": [\"what you expect\"]}\n",
    "    # },\n",
    "]\n",
    "\n",
    "print(f\"Manual eval dataset: {len(manual_eval_data)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into the final evaluation dataset\n",
    "# (If synthetic generation above didn't work, that's OK \u2014 the manual set is sufficient)\n",
    "eval_data = manual_eval_data\n",
    "print(f\"Total evaluation dataset: {len(eval_data)} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Built-in judge *(Required)*\n",
    "\n",
    "Built-in judges are pre-configured scorers that assess standard quality dimensions. They require minimal setup.\n",
    "\n",
    "We'll use `RelevanceToQuery` \u2014 it checks whether the agent's response actually addresses the user's question.\n",
    "\n",
    "**Docs:** [Built-in scorers](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/concepts/judges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import RelevanceToQuery\n",
    "\n",
    "# Define the predict function \u2014 wraps the agent for evaluation\n",
    "def predict_fn(inputs):\n",
    "    \"\"\"Run the agent and return the output string.\"\"\"\n",
    "    response = agent_executor.invoke(inputs)\n",
    "    return response[\"output\"]\n",
    "\n",
    "# Run evaluation with the built-in RelevanceToQuery judge\n",
    "print(\"Running evaluation with RelevanceToQuery...\")\n",
    "builtin_results = mlflow.genai.evaluate(\n",
    "    data=eval_data,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[RelevanceToQuery()]\n",
    ")\n",
    "\n",
    "print(\"\\nBuilt-in judge evaluation complete. Check the MLflow UI link above for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Guidelines judge *(Required)*\n",
    "\n",
    "A **guidelines judge** checks whether the response follows specific rules you define in natural language. This is great for domain-specific quality bars.\n",
    "\n",
    "You'll write rules specific to the UltraFeedback Expert agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Guidelines\n",
    "\n",
    "# Define domain-specific guidelines for the UltraFeedback Expert\n",
    "tool_citation_judge = Guidelines(\n",
    "    name=\"tool_citation\",\n",
    "    guidelines=\"The response must clearly state which tool or data source was used \"\n",
    "               \"to generate the answer. If no tool was needed, the response should \"\n",
    "               \"acknowledge that. Vague answers that don't explain their source fail.\"\n",
    ")\n",
    "\n",
    "data_grounding_judge = Guidelines(\n",
    "    name=\"data_grounding\",\n",
    "    guidelines=\"When the question is about the UltraFeedback dataset, the response \"\n",
    "               \"must include specific numbers or examples from the actual data \"\n",
    "               \"(e.g., row counts, source names, sample instructions). Generic or \"\n",
    "               \"made-up statistics fail this criterion.\"\n",
    ")\n",
    "\n",
    "# Run evaluation with both guidelines judges\n",
    "print(\"Running evaluation with Guidelines judges...\")\n",
    "guidelines_results = mlflow.genai.evaluate(\n",
    "    data=eval_data,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[tool_citation_judge, data_grounding_judge]\n",
    ")\n",
    "\n",
    "print(\"\\nGuidelines judge evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Custom judge *(Required)*\n",
    "\n",
    "A **custom judge** gives you full control over the evaluation prompt. You define the rubric, scoring criteria, and output format. This is useful when built-in and guidelines judges don't capture exactly what you care about.\n",
    "\n",
    "We'll create a custom judge that evaluates whether the agent used its tools **appropriately** \u2014 not just whether the answer was relevant, but whether the agent chose the *right* tool for the question.\n",
    "\n",
    "**Docs:** [Custom judges](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/custom-judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.judges import make_judge\n",
    "\n",
    "# Create a custom judge that evaluates tool usage appropriateness\n",
    "tool_usage_judge = make_judge(\n",
    "    name=\"tool_usage_quality\",\n",
    "    judge_prompt=\"\"\"You are evaluating an AI agent's ability to use tools appropriately.\n",
    "\n",
    "The agent has access to these tools:\n",
    "- lookup_source_info: looks up row counts and samples for a data source name\n",
    "- analyze_instruction: analyzes complexity of instruction text\n",
    "- Vector Search: finds semantically similar instructions\n",
    "\n",
    "User question: {{request}}\n",
    "Agent response: {{response}}\n",
    "\n",
    "Evaluate the agent's tool usage:\n",
    "1. Did the agent call the appropriate tool(s) for this question?\n",
    "2. If the question didn't need a tool, did the agent correctly avoid using one?\n",
    "3. Did the agent use the tool results correctly in its response?\n",
    "\n",
    "Return YES if tool usage was appropriate, NO if it was not.\n",
    "Explain your reasoning.\"\"\",\n",
    "    output_type=\"boolean\"\n",
    ")\n",
    "\n",
    "# Run evaluation with the custom judge\n",
    "print(\"Running evaluation with custom judge...\")\n",
    "custom_results = mlflow.genai.evaluate(\n",
    "    data=eval_data,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[tool_usage_judge]\n",
    ")\n",
    "\n",
    "print(\"\\nCustom judge evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Compare all judges and analyze *(Required)*\n",
    "\n",
    "Now run all judges together in a single evaluation pass and compare how they rate the same outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all judges together\n",
    "print(\"Running combined evaluation with all judges...\")\n",
    "all_results = mlflow.genai.evaluate(\n",
    "    data=eval_data,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=[\n",
    "        RelevanceToQuery(),\n",
    "        tool_citation_judge,\n",
    "        data_grounding_judge,\n",
    "        tool_usage_judge,\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\nCombined evaluation complete. See the MLflow UI for the full results table.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your analysis\n",
    "\n",
    "Review the evaluation results in the MLflow UI (click the link in the cell output above, then go to the **Evaluations** tab). Answer these questions:\n",
    "\n",
    "**8a. Which judge was strictest? Which was most lenient?**\n",
    "\n",
    "*Look at the pass/fail rates across all judges. Which one failed the most responses?*\n",
    "\n",
    "**[Your answer here]**\n",
    "\n",
    "**8b. Where did the judges disagree?**\n",
    "\n",
    "*Find specific examples where one judge passed and another failed the same response. Why did they disagree? What does this tell you about evaluation design?*\n",
    "\n",
    "**[Your answer here]**\n",
    "\n",
    "**8c. What did the evaluation reveal about your agent?**\n",
    "\n",
    "*What are the agent's strengths and weaknesses based on the evaluation? If you were to improve the agent, what would you change first \u2014 the tools, the prompt, or the LLM?*\n",
    "\n",
    "**[Your answer here]**\n",
    "\n",
    "**8d. How does this connect to the readings?**\n",
    "\n",
    "*Reference at least one of the Week 5 readings (Product Evals, Survey on Evaluation, Evals FAQ). How did the reading inform your approach to evaluation?*\n",
    "\n",
    "**[Your answer here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Clean up *(Required)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Lab complete\n",
    "\n",
    "### Required (Sections 1\u20139)\n",
    "- [ ] **Section 3:** Agent recreated and verified (tools + embeddings table + prompt).\n",
    "- [ ] **Section 4:** Evaluation dataset created with at least 8 provided + 5 of your own questions.\n",
    "- [ ] **Section 5:** Built-in judge (`RelevanceToQuery`) evaluation ran successfully.\n",
    "- [ ] **Section 6:** Guidelines judges (`tool_citation`, `data_grounding`) evaluation ran successfully.\n",
    "- [ ] **Section 7:** Custom judge (`tool_usage_quality`) evaluation ran successfully.\n",
    "- [ ] **Section 8:** Combined evaluation ran. All four analysis questions answered.\n",
    "- [ ] **Section 9:** No VS cleanup needed (embeddings persist in Delta table).\n",
    "\n",
    "**Submit:** Your executed notebook (`.ipynb` with all outputs) and the completed `SUBMISSION_5.md`.\n",
    "\n",
    "*Next week you'll generate traces, provide human feedback, and use `align()` to improve your judges.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}