{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "461cb433-8832-4f70-93f4-1a4fb8ca88d5",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "# AAI 594 — Assignment 1\n",
        "\n",
        "## Pull in data & configure Unity Catalog\n",
        "\n",
        "**In this lab you will:**\n",
        "- **Required (Sections 1–6):** Download the dataset, store it in Unity Catalog, run the Foundation Model demo, and connect your local machine via the **Databricks CLI**.\n",
        "- **Optional but strongly encouraged (Sections 7–8):** Connect **Cursor** to Databricks (Databricks Connect) and/or create an **external model endpoint** (OpenAI or Claude) with your own API key.\n",
        "- Get comfortable with the Databricks notebook and data platform.\n",
        "\n",
        "*Next week you'll perform EDA on this dataset; the course will build in complexity from there.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Install dependencies *(Required)*\n",
        "\n",
        "You'll install [`kagglehub`](https://github.com/Kaggle/kagglehub) so you can load the UltraFeedback dataset from Kaggle. The `[pandas-datasets]` extra gives you a Pandas-friendly loader.\n",
        "\n",
        "*Run the cell below. If the kernel prompts you, re-run the notebook from the top (or run the next cell manually).*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "5ee47728-3b1a-4a25-85b2-172fdce14741",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kagglehub[pandas-datasets]\n",
            "  Downloading kagglehub-0.4.2-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting kagglesdk<1.0,>=0.1.14 (from kagglehub[pandas-datasets])\n",
            "  Downloading kagglesdk-0.1.15-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging in /databricks/python3/lib/python3.12/site-packages (from kagglehub[pandas-datasets]) (24.1)\n",
            "Requirement already satisfied: pyyaml in /databricks/python3/lib/python3.12/site-packages (from kagglehub[pandas-datasets]) (6.0.2)\n",
            "Requirement already satisfied: requests in /databricks/python3/lib/python3.12/site-packages (from kagglehub[pandas-datasets]) (2.32.3)\n",
            "Collecting tqdm (from kagglehub[pandas-datasets])\n",
            "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: pandas in /databricks/python3/lib/python3.12/site-packages (from kagglehub[pandas-datasets]) (2.2.3)\n",
            "Requirement already satisfied: protobuf in /databricks/python3/lib/python3.12/site-packages (from kagglesdk<1.0,>=0.1.14->kagglehub[pandas-datasets]) (5.29.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /databricks/python3/lib/python3.12/site-packages (from pandas->kagglehub[pandas-datasets]) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /databricks/python3/lib/python3.12/site-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.12/site-packages (from pandas->kagglehub[pandas-datasets]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /databricks/python3/lib/python3.12/site-packages (from pandas->kagglehub[pandas-datasets]) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests->kagglehub[pandas-datasets]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests->kagglehub[pandas-datasets]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests->kagglehub[pandas-datasets]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests->kagglehub[pandas-datasets]) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.16.0)\n",
            "Downloading kagglesdk-0.1.15-py3-none-any.whl (160 kB)\n",
            "Downloading kagglehub-0.4.2-py3-none-any.whl (69 kB)\n",
            "Downloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
            "Installing collected packages: tqdm, kagglesdk, kagglehub\n",
            "Successfully installed kagglehub-0.4.2 kagglesdk-0.1.15 tqdm-4.67.3\n",
            "\u001b[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Install kagglehub with pandas support; %restart_python ensures the new package is available in this session\n",
        "%pip install kagglehub[pandas-datasets]\n",
        "%restart_python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Download the dataset from Kaggle *(Required)*\n",
        "\n",
        "In this lab you'll use the **LLM Human Preference Data (UltraFeedback)** dataset from Kaggle:\n",
        "\n",
        "**Dataset link:** [LLM Human Preference Data (UltraFeedback) — Kaggle](https://www.kaggle.com/datasets/thedrcat/llm-human-preference-data-ultrafeedback?select=ultrafeedback.csv)\n",
        "\n",
        "### About the dataset\n",
        "\n",
        "- **What it is:** A large-scale human preference dataset for training and evaluating language models. It contains **preference pairs**: for each prompt, multiple model responses were collected and humans (or model-based annotators) indicated which response was *chosen* (preferred) vs *rejected*.\n",
        "- **Why it matters for agentic AI:** This kind of data is used for **reward modeling** and **alignment**—teaching models to behave in ways humans prefer. Those techniques underpin safer and more controllable agents.\n",
        "- **What you're loading:** The file `ultrafeedback.csv` in this dataset. Typical columns include `source` (e.g. evol_instruct), the instruction/prompt, **chosen** vs **rejected** model outputs, and which models produced them (e.g. `chosen-model`, `rejected-model`). You'll explore the exact schema during EDA next week.\n",
        "- **Reference:** UltraFeedback-style data is widely used in the literature; the Kaggle version gives you a convenient, tabular form for analysis and experimentation in Databricks.\n",
        "\n",
        "In the cell below you'll load it as a Pandas DataFrame. You can optionally use `kagglehub.dataset_load()` (new API) instead of `load_dataset()`.\n",
        "\n",
        "**If the download fails:** Accept the dataset rules on the [Kaggle dataset page](https://www.kaggle.com/datasets/thedrcat/llm-human-preference-data-ultrafeedback) (click \"Join competition\" or \"Download\" and accept if prompted). If your environment doesn't have Kaggle credentials, set `KAGGLE_USERNAME` and `KAGGLE_KEY` (from your [Kaggle account → Settings → API](https://www.kaggle.com/docs/api#authentication)) in the notebook or compute environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "16417dc7-bf61-46ab-999a-4c99919e481b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/spark-2f8f00ca-bd01-4beb-b161-e2/.ipykernel/3659/command-7060154211831815-4268597593:10: DeprecationWarning: Use dataset_load() instead of load_dataset(). load_dataset() will be removed in a future version.\n",
            "  df = kagglehub.load_dataset(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading to /home/spark-2f8f00ca-bd01-4beb-b161-e2/.cache/kagglehub/datasets/thedrcat/llm-human-preference-data-ultrafeedback/versions/2/ultrafeedback.csv...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0.00/101M [00:00<?, ?B/s]\r  1%|          | 1.00M/101M [00:00<00:23, 4.49MB/s]\r  2%|▏         | 2.00M/101M [00:00<00:18, 5.72MB/s]\r  3%|▎         | 3.00M/101M [00:00<00:16, 6.29MB/s]\r  4%|▍         | 4.00M/101M [00:00<00:15, 6.67MB/s]\r  5%|▍         | 5.00M/101M [00:00<00:14, 7.01MB/s]\r  6%|▌         | 6.00M/101M [00:00<00:13, 7.35MB/s]\r  7%|▋         | 7.00M/101M [00:01<00:12, 7.60MB/s]\r  8%|▊         | 8.00M/101M [00:01<00:12, 7.91MB/s]\r  9%|▉         | 9.00M/101M [00:01<00:11, 8.07MB/s]\r 10%|▉         | 10.0M/101M [00:01<00:11, 8.30MB/s]\r 12%|█▏        | 12.0M/101M [00:01<00:09, 10.2MB/s]\r 15%|█▍        | 15.0M/101M [00:01<00:05, 15.4MB/s]\r 21%|██        | 21.0M/101M [00:01<00:04, 19.3MB/s]\r 23%|██▎       | 23.0M/101M [00:02<00:04, 19.4MB/s]\r 28%|██▊       | 28.0M/101M [00:02<00:02, 26.7MB/s]\r 31%|███       | 31.0M/101M [00:02<00:02, 27.3MB/s]\r 34%|███▎      | 34.0M/101M [00:02<00:02, 26.0MB/s]\r 40%|███▉      | 40.0M/101M [00:02<00:01, 34.4MB/s]\r 44%|████▎     | 44.0M/101M [00:02<00:01, 34.8MB/s]\r 49%|████▊     | 49.0M/101M [00:02<00:01, 38.1MB/s]\r 56%|█████▌    | 56.0M/101M [00:02<00:01, 46.4MB/s]\r 61%|██████    | 61.0M/101M [00:03<00:01, 33.8MB/s]\r 68%|██████▊   | 69.0M/101M [00:03<00:00, 42.2MB/s]\r 76%|███████▋  | 77.0M/101M [00:03<00:00, 49.5MB/s]\r 84%|████████▍ | 85.0M/101M [00:03<00:00, 57.3MB/s]\r 90%|█████████ | 91.0M/101M [00:03<00:00, 58.4MB/s]\r100%|██████████| 101M/101M [00:03<00:00, 59.2MB/s] \r100%|██████████| 101M/101M [00:03<00:00, 28.4MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting zip of ultrafeedback.csv...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 records:           source  ...    rejected-model\n",
            "0  evol_instruct  ...         alpaca-7b\n",
            "1  evol_instruct  ...        vicuna-33b\n",
            "2  evol_instruct  ...        pythia-12b\n",
            "3  evol_instruct  ...  llama-2-13b-chat\n",
            "4  evol_instruct  ...          starchat\n",
            "\n",
            "[5 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "# Import kagglehub and the adapter that returns Pandas DataFrames\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Which file from the dataset to load (this dataset has ultrafeedback.csv)\n",
        "file_path = \"ultrafeedback.csv\"\n",
        "\n",
        "# Load the latest version of the dataset as a Pandas DataFrame.\n",
        "# Dataset: \"thedrcat/llm-human-preference-data-ultrafeedback\"\n",
        "# For more options (e.g. sql_query, pandas_kwargs), see:\n",
        "# https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        "df = kagglehub.load_dataset(\n",
        "    KaggleDatasetAdapter.PANDAS,\n",
        "    \"thedrcat/llm-human-preference-data-ultrafeedback\",\n",
        "    file_path,\n",
        ")\n",
        "\n",
        "print(\"First 5 records:\", df.head())\n",
        "print(\"\\nShape:\", df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Configure Unity Catalog *(Required)*\n",
        "\n",
        "**What is Unity Catalog?**\n",
        "[Unity Catalog](https://docs.databricks.com/en/data-governance/unity-catalog/index.html) is Databricks' **governance layer** for data and AI assets. It organizes everything in a hierarchy: **catalogs** (top-level containers) → **schemas** (namespaces inside a catalog) → **tables** (and views). It provides fine-grained access control, audit logging, and data lineage so you know who can see what and where data came from. All tables you'll use in this course live under Unity Catalog (e.g. `main.default.assignment_file` = catalog `main`, schema `default`, table `assignment_file`).\n",
        "\n",
        "In the next cell you'll ensure a catalog and schema exist (e.g. `main.default`) so you can write your Delta table into them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "9710bb73-5c68-4554-ad1b-0492b06ca296",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the default catalog and schema if they don't already exist\n",
        "# (Safe to run repeatedly; IF NOT EXISTS avoids errors.)\n",
        "spark.sql(\"CREATE CATALOG IF NOT EXISTS main\")\n",
        "spark.sql(\"CREATE SCHEMA IF NOT EXISTS main.default\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Write the dataset to a Delta table in Unity Catalog *(Required)*\n",
        "\n",
        "**What is a Delta table?**\n",
        "[Delta Lake](https://docs.databricks.com/en/delta/index.html) is an open-source storage format (built on Parquet) that adds **ACID transactions** (reads and writes are consistent), **time travel** (query or restore earlier versions of the table), and **efficient upserts and deletes**. In Databricks, when you write a table with `format(\"delta\")`, it becomes a Delta table—the default and recommended way to store tabular data in the lakehouse. You get reliability and versioning without managing it yourself.\n",
        "\n",
        "In the next cell you'll convert the Pandas DataFrame to a Spark DataFrame and write it as a Delta table at `main.default.assignment_file` (catalog.schema.table).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "83fb56c7-e45e-4f06-8a47-67621e7479cb",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Unity Catalog location: catalog.schema.table\n",
        "catalog = \"main\"\n",
        "schema = \"default\"\n",
        "table_name = \"assignment_file\"\n",
        "\n",
        "# Convert Pandas DataFrame to Spark and write as Delta\n",
        "# mode(\"overwrite\") replaces the table if it exists (good for re-running the lab)\n",
        "assignment_file = spark.createDataFrame(df)\n",
        "assignment_file.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{catalog}.{schema}.{table_name}\")\n",
        "\n",
        "# Verify: query the table (optional)\n",
        "display(spark.table(f\"{catalog}.{schema}.{table_name}\").limit(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Demo: LLM call via Foundation Model APIs *(Required)*\n",
        "\n",
        "**What are Foundation Model APIs?**\n",
        "[Foundation Model APIs](https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html) are Databricks-hosted large language models (LLMs) that you can call **without deploying or managing your own model**. Databricks runs the model; you send a request (e.g. a chat message) and get a response. You can use **pay-per-token** (no upfront capacity) or **provisioned throughput** (reserved capacity for production).\n",
        "\n",
        "**Calling via the MLflow Deployments SDK:** Below you'll use the [MLflow Deployments SDK](https://docs.databricks.com/en/mlflow/mlflow-deployments.html) (`mlflow.deployments.get_deploy_client(\"databricks\")`) to call the Foundation Model. When you run this in a Databricks notebook, the SDK uses your **workspace authentication**—no API key or env vars needed. You pass the model name as the `endpoint` and your messages in the `inputs` dict. This is the same pattern you'll use later with MLflow for tracking and the model registry.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo: call databricks-gpt-oss-120b via the MLflow Deployments SDK (Foundation Model API)\n",
        "# In a Databricks notebook, the client uses your workspace auth—no API key needed.\n",
        "import mlflow.deployments\n",
        "\n",
        "client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
        "\n",
        "chat_response = client.predict(\n",
        "    endpoint=\"databricks-gpt-oss-120b\",\n",
        "    inputs={\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a concise teaching assistant for an agentic AI course.\"},\n",
        "            {\"role\": \"user\", \"content\": \"In 1–2 sentences, what is agentic AI and why might we store training data in Unity Catalog?\"},\n",
        "        ],\n",
        "        \"max_tokens\": 256,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Response has the same structure as the Foundation Model REST API (OpenAI-style)\n",
        "print(chat_response[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Connect your local machine: Databricks CLI *(Required)*\n",
        "\n",
        "The **Databricks CLI** lets you run Databricks commands from your laptop or dev machine—manage workspace resources, run jobs, sync files, and use the same auth as in the browser. You'll use it to keep local code and Databricks in sync and to script common tasks.\n",
        "\n",
        "**Workspace URL:** Your workspace host is `https://<your-workspace>.cloud.databricks.com`. Here **&lt;your-workspace&gt;** is the prefix—everything *before* `.cloud.databricks.com`. Example: for `https://dbc-xxxx-xxxx.cloud.databricks.com`, the prefix is `dbc-xxxx-xxxx`.\n",
        "\n",
        "**Do this on your local machine** (not in this notebook):\n",
        "\n",
        "1. **Install the CLI**  \n",
        "   - macOS: `brew tap databricks/tap` then `brew install databricks`  \n",
        "   - See [Install or update the Databricks CLI](https://docs.databricks.com/en/dev-tools/cli/install.html) for Windows and other options.\n",
        "\n",
        "2. **Sign in with OAuth**  \n",
        "   - Run: `databricks auth login --host https://<your-workspace>.cloud.databricks.com`  \n",
        "   - Replace `<your-workspace>` with your workspace prefix (e.g. `dbc-xxxx-xxxx`). Your browser will open; sign in to Databricks. The CLI creates a profile in `~/.databrickscfg` and stores your OAuth credentials there.\n",
        "\n",
        "3. **Verify**  \n",
        "   - Run `databricks -v` (check version).  \n",
        "   - Run `databricks auth profiles` (list profiles).  \n",
        "   - Run `databricks workspace list /` to confirm the CLI can reach your workspace.\n",
        "\n",
        "**Docs:** [Databricks CLI](https://docs.databricks.com/en/dev-tools/cli/index.html) · [Authentication](https://docs.databricks.com/en/dev-tools/cli/authentication.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Connect Cursor to Databricks *(Optional, strongly encouraged)*\n",
        "\n",
        "Follow the steps below to set up **Cursor with [Databricks Connect](https://docs.databricks.com/en/dev-tools/databricks-connect/index.html)** on your local machine. Databricks Connect lets you develop in Cursor while running Spark workloads on Databricks compute. Once this is in place, you can optionally add MCP (e.g. Unity Catalog functions) for more IDE integration; see the links at the end.\n",
        "\n",
        "**Instructions below match:** [Cursor with Databricks: AI Enhanced Development](https://dustinvannoy.com/2025/09/29/cursor-with-databricks-ai-enhanced-development/) (Dustin Vannoy).\n",
        "\n",
        "**Installation and authentication:**\n",
        "\n",
        "1. **Install Cursor and create an account**\n",
        "   - Go to [cursor.com](https://cursor.com) and download Cursor for your OS. Install it (e.g. on macOS: open the .dmg and drag Cursor to Applications; on Windows: run the .exe). See [Cursor installation docs](https://docs.cursor.com/get-started/installation) if you need help.\n",
        "   - On first launch, sign in or create a Cursor account. The 14-day trial includes what you need to get started; after that, Pro or a higher tier is useful for regular use.\n",
        "\n",
        "2. **Install the Databricks extension within Cursor**\n",
        "   - In Cursor, open the Extensions view and search for **Databricks**. Install the official Databricks extension.\n",
        "   - This extension manages your Databricks connection, provides run options, and supports Asset Bundles integration.\n",
        "\n",
        "3. **Configure your Databricks profile using the extension's setup wizard**\n",
        "   - Use the extension to add a Databricks connection. You'll need your **workspace URL** (from your Databricks workspace address) and an **authentication method**. OAuth is the preferred method; an access token will also work well.\n",
        "\n",
        "**The critical configuration detail:**\n",
        "\n",
        "4. **Name your profile \"DEFAULT\" (in all caps)**\n",
        "   - This eliminates countless issues with AI tools. Databricks Connect automatically looks for the default profile, and your AI assistant won't need explicit instructions about which profile to use. When you switch environments, change which profile is the default rather than reconfiguring everything.\n",
        "\n",
        "5. **Set serverless compute in the profile**\n",
        "   - We use the free edition, so compute is always serverless. Edit the profile configuration file at `~/.databrickscfg` and set `serverless_compute_id = auto`.\n",
        "\n",
        "**Virtual environment setup:**\n",
        "\n",
        "6. **Use a virtual environment and install Databricks Connect**\n",
        "   - Use a virtual environment for Python development (create one manually or let Cursor create it). Install Databricks Connect in that environment. The Databricks extension can handle this automatically; you may need to adjust the version to match your Databricks runtime.\n",
        "\n",
        "7. **Verify the connection**\n",
        "   - Run simple Databricks Connect code (e.g. create a Spark session and run a trivial query). The built-in Cursor run button works well and mimics how the AI agent executes code.\n",
        "\n",
        "**Optional — MCP (Unity Catalog, etc.):** For access to Unity Catalog functions, SQL, or other MCP tools from Cursor, see [Connect non-Databricks clients to Databricks MCP servers](https://docs.databricks.com/aws/en/generative-ai/mcp/connect-external-services). You can add the Databricks MCP server (PAT or OAuth via mcp-remote) once this Connect setup is working.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. Create an external model endpoint (OpenAI or Claude) *(Optional, strongly encouraged)*\n",
        "\n",
        "**External models** are third-party LLMs (OpenAI, Anthropic, etc.) that you call *through* Databricks. Databricks stores the API credentials in one place so you don’t put keys in code or notebooks. You create a **model serving endpoint** that forwards requests to the provider and returns responses in the same OpenAI-style format you used in Section 5.\n",
        "\n",
        "**You need your own API keys for this step.** If you don't already have one, get an API key from [OpenAI](https://platform.openai.com/api-keys) or [Anthropic](https://console.anthropic.com/settings/keys) and configure it in Databricks (see below). There is no built-in external-model key in the free edition—you must supply and configure your own.\n",
        "\n",
        "**Why use this:** Centralized credential management, consistent query interface (same `predict()` or chat API), and the option to add [Mosaic AI Gateway](https://docs.databricks.com/aws/en/generative-ai/external-models/) for rate limiting and guardrails.\n",
        "\n",
        "Below is an example using the **MLflow Deployments SDK** to create an endpoint that calls **OpenAI** (you can swap in **Anthropic** by changing the provider and config). Store your API key in [Databricks Secrets](https://docs.databricks.com/en/security/secrets/index.html) and reference it as `{{secrets/<scope>/<key>}}`. For a quick test you can use plaintext (not for production)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create an external model endpoint that calls OpenAI (or Anthropic) using the MLflow Deployments SDK.\n",
        "# You must configure your own OpenAI or Anthropic API key (see Section 8 text above) if not already done.\n",
        "# Run this once; then query the endpoint by name with client.predict(endpoint=\"<name>\", inputs={...}).\n",
        "# Store your API key in Databricks Secrets: https://docs.databricks.com/en/security/secrets/index.html\n",
        "# Reference as \"{{secrets/<scope>/<key>}}\". For a one-off test only, you can use openai_api_key_plaintext (not for production).\n",
        "\n",
        "import mlflow.deployments\n",
        "\n",
        "client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
        "\n",
        "# Example: OpenAI chat endpoint. Replace the secret scope and key with your own, or use openai_api_key_plaintext for testing.\n",
        "client.create_endpoint(\n",
        "    name=\"assignment1-openai-chat\",  # use a unique name per workspace\n",
        "    config={\n",
        "        \"served_entities\": [\n",
        "            {\n",
        "                \"name\": \"openai-chat\",\n",
        "                \"external_model\": {\n",
        "                    \"name\": \"gpt-4o-mini\",  # or gpt-4o, gpt-3.5-turbo, etc.\n",
        "                    \"provider\": \"openai\",\n",
        "                    \"task\": \"llm/v1/chat\",\n",
        "                    \"openai_config\": {\n",
        "                        \"openai_api_key\": \"{{secrets/<your-scope>/openai_api_key}}\",\n",
        "                        # \"openai_api_key_plaintext\": \"<your-key>\",  # testing only\n",
        "                    },\n",
        "                },\n",
        "            }\n",
        "        ]\n",
        "    },\n",
        ")\n",
        "\n",
        "# To use Anthropic instead, use provider \"anthropic\", name e.g. \"claude-3-5-sonnet-20241022\",\n",
        "# task \"llm/v1/chat\", and anthropic_config with anthropic_api_key (or secret reference).\n",
        "# See: https://docs.databricks.com/aws/en/generative-ai/external-models/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the endpoint is created and in **Ready** state (check **Serving** in the left sidebar), you can call it with the same MLflow Deployments SDK pattern: `client.predict(endpoint=\"assignment1-openai-chat\", inputs={\"messages\": [...], \"max_tokens\": 256})`. The response format is the same as in Section 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Getting comfortable with Databricks\n",
        "\n",
        "As you work through this course, you'll use several Databricks features over and over. Here's a short guide so you know what to expect and where to look.\n",
        "\n",
        "### What you've seen (or will see) in this lab\n",
        "\n",
        "- **Unity Catalog + Delta tables** (Sections 3–4): Your table `main.default.assignment_file` is the kind of \"source of truth\" we'll use for data that agents and models read from or write to. Get used to the three-level name: *catalog.schema.table*.\n",
        "- **Foundation Model APIs** (Section 5): You called a hosted LLM without deploying anything. We'll use these for prompts, tool-calling, and multi-turn flows. Try the same model in **Serving** → **AI Playground** in the left sidebar to compare with the API.\n",
        "- **Databricks CLI** (Section 6, *required*): Use it from your local machine to manage workspaces, run jobs, and keep code in sync—no need to do everything in the browser.\n",
        "- **Cursor and Databricks Connect** (Section 7, *optional but strongly encouraged*): Setting up Cursor with the Databricks extension and DEFAULT profile gives you local development with Databricks compute; you can optionally add MCP later for Unity Catalog and SQL in your IDE.\n",
        "- **External model endpoints** (Section 8, *optional but strongly encouraged*): OpenAI/Claude (and other providers) through Databricks with credentials in one place; same `predict()` interface as Foundation Models.\n",
        "- **Notebooks and Spark**: Running cells in order, using `spark.sql(...)` and DataFrames, and (later) `%run` to share code are the basics. **Workflows** (jobs) turn notebooks into scheduled or triggered steps—useful when you orchestrate agents.\n",
        "\n",
        "### MLflow: experiments, models, and agent traces\n",
        "\n",
        "**MLflow** on Databricks is the platform for the full ML lifecycle. You'll use it a lot in this course:\n",
        "\n",
        "- **Tracking**: Log parameters, metrics, and artifacts for each run so you can compare experiments (e.g. different prompts or models).\n",
        "- **Model Registry**: Register and version models, then promote them from staging to production. Fits nicely with Unity Catalog.\n",
        "- **AI/agent evaluation and tracing**: For LLM apps and agents, MLflow lets you trace requests (inputs, outputs, tool calls, latency) and evaluate quality. When something goes wrong, you can inspect the trace instead of guessing.\n",
        "\n",
        "Docs: [MLflow on Databricks](https://docs.databricks.com/en/mlflow/index.html). You'll use Tracking and the registry as you build and compare agents.\n",
        "\n",
        "### Agent Bricks: framework for production agents\n",
        "\n",
        "**Agent Bricks** is Databricks' framework for building and optimizing production AI agents *on your data*. It automates a lot of the heavy lifting:\n",
        "\n",
        "- **Auto-optimization**: It can generate domain-specific synthetic data, create task-aware benchmarks, and tune model choice, prompts, and config to balance quality and cost—so you spend less time on manual trial-and-error.\n",
        "- **Evaluation**: Built-in evaluation and LLM-as-judge workflows so you can compare agent versions and know when a change is an improvement.\n",
        "- **Use cases**: Information extraction from documents, knowledge assistants over your data, document classification, multi-agent systems, and more.\n",
        "\n",
        "You'll work with Agent Bricks when we move from single LLM calls to full agent workflows. Overview: [Agent Bricks](https://www.databricks.com/product/artificial-intelligence/agent-bricks).\n",
        "\n",
        "### Things to try on your own\n",
        "\n",
        "- In the **Data** UI, find `main.default.assignment_file`, open it, and check the **Lineage** tab to see where the table came from.\n",
        "- In a **SQL** notebook (or a new cell), run `SELECT * FROM main.default.assignment_file LIMIT 10` and confirm you get the same data.\n",
        "- **Re-run this notebook** from top to bottom and confirm the table is overwritten; later we'll use Delta time travel to look at older versions.\n",
        "\n",
        "### Where to look things up\n",
        "\n",
        "- [Foundation Model APIs](https://docs.databricks.com/en/machine-learning/model-serving/score-foundation-models.html)\n",
        "- [MLflow Deployments SDK](https://docs.databricks.com/en/mlflow/mlflow-deployments.html)\n",
        "- [Unity Catalog](https://docs.databricks.com/en/data-governance/unity-catalog/index.html)\n",
        "- [Delta Lake](https://docs.databricks.com/en/delta/index.html)\n",
        "- [MLflow on Databricks](https://docs.databricks.com/en/mlflow/index.html)\n",
        "- [Databricks CLI](https://docs.databricks.com/en/dev-tools/cli/index.html)\n",
        "- [Databricks Connect](https://docs.databricks.com/en/dev-tools/databricks-connect/index.html)\n",
        "- [Databricks Secrets](https://docs.databricks.com/en/security/secrets/index.html)\n",
        "- [External Models / AI Gateway](https://docs.databricks.com/aws/en/generative-ai/external-models/)\n",
        "- [Agent Bricks](https://www.databricks.com/product/artificial-intelligence/agent-bricks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it yourself:** Run the cell below to explore the table and the workspace. Change the SQL or DataFrame code and re-run to get comfortable with Spark and Unity Catalog.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 1. Query the table with SQL (same as in the Data UI or a SQL notebook) ---\n",
        "display(spark.sql(\"SELECT * FROM main.default.assignment_file LIMIT 10\"))\n",
        "\n",
        "# --- 2. Inspect the schema (column names and types) ---\n",
        "spark.table(\"main.default.assignment_file\").printSchema()\n",
        "\n",
        "# --- 3. Same table as a Spark DataFrame: filter, select, count ---\n",
        "tbl = spark.table(\"main.default.assignment_file\")\n",
        "print(\"Row count:\", tbl.count())\n",
        "# Example: show one column (change to any column name you see in the schema)\n",
        "# display(tbl.select(\"source\").limit(5))\n",
        "\n",
        "# --- 4. See where your table lives: schemas in the main catalog ---\n",
        "display(spark.sql(\"SHOW SCHEMAS IN main\"))\n",
        "# To list all catalogs: display(spark.sql(\"SHOW CATALOGS\"))\n",
        "\n",
        "# --- 5. Optional: list the workspace filesystem root (dbutils is handy in Databricks) ---\n",
        "# display(dbutils.fs.ls(\"/\"))\n",
        "\n",
        "# --- 6. Teaser: Delta keeps history of table changes (time travel — we'll use this later) ---\n",
        "# display(spark.sql(\"DESCRIBE HISTORY main.default.assignment_file\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Lab complete\n",
        "\n",
        "**Required:**\n",
        "- **Sections 1–4:** The table `main.default.assignment_file` exists in Unity Catalog and the exploration cell above shows rows and schema.\n",
        "- **Section 5:** The Foundation Model demo cell ran and returned a short LLM response.\n",
        "- You've run the exploration cell (SQL, schema, DataFrame) and optionally changed the SQL or DataFrame code to try your own queries.\n",
        "- **Section 6:** Databricks CLI installed and signed in on your local machine (`databricks auth login --host https://<workspace>.cloud.databricks.com`).\n",
        "\n",
        "**Optional but strongly encouraged:**\n",
        "- **Section 7:** Cursor (or another IDE) set up with the Databricks extension and a DEFAULT profile using serverless compute.\n",
        "- **Section 8:** An external model endpoint (OpenAI or Anthropic) created with your own API key and at least one successful `predict()` call.\n",
        "\n",
        "*Next week you'll run EDA on this dataset.*\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "4"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "AAI_594_Assignment_1_Lab",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
